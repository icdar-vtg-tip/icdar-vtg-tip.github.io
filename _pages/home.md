---
layout: project
urltitle:  "Visual Text Generation and Text Image Processing"
title: "Visual Text Generation and Text Image Processing"
categories: icdar, workshop, computer vision, computer graphics, deep learning
permalink: /
favicon: /static/img/ico/favicon.png
bibtex: true
paper: true
acknowledgements: ""
description: "Website for the Workshop on Visual Text Generation and Text Image Processing at ICDAR 2025"
---

<br>
<div class="row">
  <div class="col-xs-12">
    <center><h2>Workshop on</h2></center>
    <center><h1>Visual Text Generation and Text Image Processing</h1></center>
    <center><h2>ICDAR 2025 Workshop</h2></center>
    <center><span style="font-weight:400;">September 16-21, 2025 @ Wuhan, Hubei, China</span></center>
    <center><span style="color:#e74c3c;font-weight:400;"></span></center>
    <br />
  </div>
</div>

<hr>

<!--<b>ðŸ“¢ Calling all researchers and enthusiasts! ðŸš€ Join our thrilling fine-grained 3D part labeling challenge built on the Amazon Berkeley Objects (ABO) Dataset: <a href="https://eval.ai/web/challenges/challenge-page/2027/overview" target="_blank">https://eval.ai/web/challenges/challenge-page/2027/overview</a>.</b>-->
<!-- <b>ðŸ“¢ Live Q&A on Slido: <a href="https://app.sli.do/event/iYUGKVgj6AVdjGhY5dzvAd" target="_blank">https://tinyurl.com/3DVeComm-slido</a> (use it to ask questions for presentations and panel discussion).</b> <br/>
<b>ðŸ“¢ Remote presentations on Zoom: <a href="https://sfu.zoom.us/j/82710628380?pwd=OQaJJGWXuRr7IYakyPd8k6Em6iUAeg.1" target="_blank">https://tinyurl.com/3DVeComm-zoom</a>.</b> -->


<!-- <b>Join live stream <a href="https://live.allintheloop.net/Agenda/ortra/ortraECCV2022/View_agenda/236653">here</a> (ECCV registration required).</b>

<b>Submit questions to the authors of the accepted papers: <a href="https://forms.gle/FFFVHVeTtcVkWg2n8">https://forms.gle/FFFVHVeTtcVkWg2n8</a>.</b>

<b>Submit questions for the closing panel discussion using this google form: <a href="https://forms.gle/XADQAVR8HNavtVRj6">https://forms.gle/XADQAVR8HNavtVRj6</a>.</b>


 <b>Please give us your feedback on how the workshop went using this Google form: <a href='https://forms.gle/utMpEnF4hmUcR19S7'>https://forms.gle/utMpEnF4hmUcR19S7</a>.</b> -->

<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      Visual text generation and text image preprocessing are two fundamental areas that play crucial roles in modern visual text analysis systems and directly impact the performance of downstream tasks such as OCR, information extraction, and visual text understanding.
      Visual text generation addresses the critical challenge of data scarcity by creating diverse, high-quality synthetic datasets [1, 2]. This not only reduces the cost and time of manual data collection but also enables the creation of comprehensive training sets that cover various visual text types and edge cases [3, 4]. Text image preprocessing [5], serving as the foundation of reliable visual text analysis pipelines, tackles real-world challenges such as low resolution [6, 7], uneven illumination [8, 9], and geometric distortions [10, 11]. These techniques are essential for handling various visual text conditions, from documents to scene images.
      The synergy of these topics to visual text analysis is becoming increasingly important: visual text generation techniques help create better training data, while advanced text image preprocessing techniques improve text quality in real conditions, facilitating model recognition. This workshop aims to bring together researchers and practitioners to work on these two topics and foster innovations in visual text analysis.
      <!-- This workshop aims to bring together researchers working on 3D computer vision and graphics for eCommerce, with a focus on the three topics: 
      (1) 3D shape/scene understanding and generation e.g. semantic segmentation, affordance and motion, multi-view reconstruction; 
      (2) Digital human and fashion e.g. virtual try-ons and personalized fashion recommendation;
      (3) Foundation-model-assisted reasoning e.g. shape/scene synthesis from texts, language grounding in 3D models and diffusion-based 3D generative models. We successfully hosted the <a href="/iccv2023/">the first 3DV in eCommerce workshop at ICCV 2023</a> which was very well-received and inspiring to the audience. In this second workshop, we are inviting a fully new line of speakers including 3 keynote presentations from academia and 3 talks from industry experts.  -->
     <!--This workshop aims to bring together researchers working on generative models of 3D shapes and scenes with researchers and practitioners who use these generative models in a variety of research areas. For our purposes, we define "generative model" to include methods that synthesize geometry unconditionally as well as from sensory inputs (e.g. images), language, or other high-level specifications. Vision tasks that can benefit from such models include scene classification and segmentation, 3D reconstruction, human activity recognition, robotic visual navigation, question answering, and more.-->
    </p>
  </div>
</div> <br>







<div class="row">
  <div class="col-xs-12">
    <h2>Schedule</h2>
    <p>All times in Beijing Time (UTC+08:00)</p>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr>
          <td>Time</td>
          <td>Events</td>
          <td></td>
        </tr>
        <tr>
          <td>13:50 - 14:00</td>
          <td>Opening Remarks</td>
          <td></td>
        </tr>
        <tr>
          <td>14:00 - 14:40</td>
          <td>Dr.A
          <br/>
          <i>Invited Talk 1 : AAAA</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>14:40 - 15:20</td>
          <td>Prof.B
          <br/>
          <i>Invited Talk 2 : BBB</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>15:20 - 16:00</td>
          <td>Contributed Talks (best/ runner-up paper talks)</td>
          <td></td>
        </tr>
        <tr>
          <td>16:00 - 16:20</td>
          <td>Coffee break</td>
          <td></td>
        </tr>
        <tr>
          <td>16:20 - 17:40</td>
          <td>Poster or oral Session
          </td>
          <td></td>
        </tr>
      </tbody>
    </table>
  </div>
</div>
<br>



<div class="row" id="call">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
    <p>Acceptable submission topics may include but are not limited to:</p>
  </div>
</div>

<!-- <div class="row">
  <div class="col-xs-12">
    <ol style="line-height: 1.4; margin-top: 15px;">
      <li style="margin-bottom: 8px;">GANs-based and Diffusion-based models for text image synthesis.</li>
      <li style="margin-bottom: 8px;">Layout-aware document image generation.</li>
      <li style="margin-bottom: 8px;">Real-synthetic domain gap analysis.</li>
      <li style="margin-bottom: 8px;">Text generation model benchmarking.</li>
      <li style="margin-bottom: 8px;">Image text removal, editing, style transfer.</li>
      <li style="margin-bottom: 8px;">Shadow, ink, and watermark removal of text image.</li>
      <li style="margin-bottom: 8px;">Illumination correction, deblurring, and binarization of text image.</li>
      <li style="margin-bottom: 8px;">Text image super-resolution.</li>
      <li style="margin-bottom: 8px;">Document image dewarping.</li>
      <li style="margin-bottom: 8px;">Text segmentation.</li>
      <li style="margin-bottom: 0;">Tampered text detection.</li>
    </ol>
  </div>
</div><br> -->

<div class="row">
  <div class="col-xs-12">
    <ul class="list-unstyled" style="line-height: 1.4; margin-top: 15px;">
      <li class="task-point"> GANs-based and Diffusion-based models for text image synthesis</li>
      <li class="task-point"> Layout-aware document image generation</li>
      <li class="task-point"> Real-synthetic domain gap analysis</li>
      <li class="task-point"> Text generation model benchmarking</li>
      <li class="task-point"> Image text removal, editing, style transfer</li>
      <li class="task-point"> Shadow, ink, and watermark removal of text image</li>
      <li class="task-point"> Illumination correction, deblurring, and binarization of text image</li>
      <li class="task-point"> Text image super-resolution</li>
      <li class="task-point"> Document image dewarping</li>
      <li class="task-point"> Text segmentation</li>
      <li class="task-point"> Tampered text detection</li>
    </ul>
  </div>
</div><br>

<style>
.task-point {
  margin-bottom: 10px;
  padding-left: 20px;
  position: relative;
}
.task-point:before {
  content: "â–¸";
  color: #2c7be5;
  position: absolute;
  left: 0;
  font-size: 1.1em;
}
</style>



<div class="row">
  <div class="col-xs-12">
    <h2>Submission</h2>
    <p class="submission-guide">
    This workshop invites original contributions in both theoretical and applied research domains. 
    All submissions must adhere to the formatting guidelines specified on the 
    <a href="https://www.icdar2025.com/home" target="_blank" class="text-primary">ICDAR 2025 official website</a>. 
    Paper length is limited to <strong>15 pages</strong> (excluding references) and must comply with 
    our double-blind review requirements:
  </p>

  <ul class="list-unstyled">
    <li class="task-point">Remove all author identifiers (names, affiliations, etc.) from the manuscript</li>
    <li class="task-point">Cite previous work in third-person format to avoid identity disclosure</li>
    <li class="task-point">Omit acknowledgments section in initial submissions</li>
  </ul>

  <p class="submission-process">
    Submissions will be accepted through the workshop's 
    <a href="#" class="text-primary">CMT submission portal</a> 
    (activation date to be announced). At least one author of each accepted paper 
    must complete workshop registration to present the work. Detailed submission 
    procedures are available on the 
    <a href="https://www.icdar2025.com/home" target="_blank" class="text-primary">ICDAR 2025 guidelines portal</a>.
</p>
  </div>
</div><br>


<div class="row">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
    <ul class="list-unstyled">
    <li class="task-point">Submission Deadline: April 15, 2025 </li> 
    <li class="task-point">Decisions Announced: May 15, 2025 </li> 
    <li class="task-point">Camera Ready Deadline: June 1, 2025 </li> 
    <li class="task-point">Workshop: September 20, 2025 </li>
    </ul>
  </div>
</div><br>


<div class="row">
  <div class="col-xs-12">
    <h2>Publication</h2>
    <p>Accepted papers will be published in the ICDAR 2025 proceedings (workshop).</p>
  </div>
</div><br>



<!--<br>
<div class="row" id="accepted">
  <div class="col-md-12">
    <h2>Accepted Papers</h2>
  </div>
</div>

<div class="row text-center">
  <div class="col-md-12">
    <hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0001-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>3D GAN Inversion for Controllable Portrait Image Animation</a></span>
    <br/>
    <i>Connor Z. Lin, David B. Lindell, Eric R. Chan, Gordon Wetzstein</i>
    <br/>
    <a href='https://drive.google.com/file/d/18qcR7WjImq_wRpfLf2aI7cRb62JyMppY/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1ZEHbONpIk8tGnCY3PWB3gpw-Cqf-_fLt/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1HkDUt1z7M_Bv5THurz8-wewoqqkg7HFo/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0002-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>3DLatNav: Navigating Generative Latent Spaces for Semantic-Aware 3D Object Manipulation</a></span>
    <br/>
    <i>Amaya Dharmasiri, Dinithi Dissanayake, Mohamed Afham, Isuru Dissanayake, Ranga Rodrigo, Kanchana Thilakarathna</i>
    <br/>
    <a href='https://drive.google.com/file/d/1ZnObAPLwCYvUCqyreMr7dWOlWEvcEL4W/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1rD_YGMNfUkVA_7-RJ6SX8y_rjPZYZlbU/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0003-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>3D Semantic Label Transfer and Matching in Human-Robot Collaboration</a></span>
    <br/>
    <i>Szilvia Szeier, BenjÃ¡min Baffy, GÃ¡bor Baranyi, Joul Skaf, LÃ¡szlÃ³ KopÃ¡csi, Daniel Sonntag, GÃ¡bor SÃ¶rÃ¶s, and AndrÃ¡s LÅ‘rincz</i>
    <br/>
    <a href='https://drive.google.com/file/d/1GD065M4qj2BhT6ujZEv_kMTFTmBYWL6C/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15GsMVDqnVBQnmg-bIifgY8gENf-xtAn0/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0004-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Generative Multiplane Images: Making a 2D GAN 3D-Aware</a></span>
    <br/>
    <i>Xiaoming Zhao, Fangchang Ma, David GÃ¼era, Zhile Ren, Alexander G. Schwing, Alex Colburn</i>
    <br/>
    <a href='https://drive.google.com/file/d/13OVLhihZ5xQEuY_tTu94fZTu_U7WXOpO/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1H4X3FnV2GLhdxc4jJQD45T1EweLb3lZC/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0005-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Intrinsic Neural Fields: Learning Functions on Manifolds</a></span>
    <br/>
    <i>Lukas Koestler, Daniel Grittner, Michael Moeller, Daniel Cremers, Zorah LÃ¤hner</i>
    <br/>
    <a href='https://drive.google.com/file/d/1gfRpZL1HzAkyAVqnD-bWjWTPSNtBdlhk/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/19g5Nq3YIg8KdEkG77QN0QQpORgVeHH39/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0006-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Learning Joint Surface Atlases</a></span>
    <br/>
    <i>Theo Deprelle, Thibault Groueix, Noam Aigerman, Vladimir G. Kim, Mathieu Aubry</i>
    <br/>
    <a href='https://drive.google.com/file/d/1udFGRnASw9iLDf7PyKMCr_-oZOkU9msR/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1xlEOAWprb1TDx54MNKCVA-lx3uWXDvBn/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0007-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Learning Neural Radiance Fields from Multi-View Geometry</a></span>
    <br/>
    <i>Marco Orsingher, Paolo Zani, Paolo Medici, Massimo Bertozzi</i>
    <br/>
    <a href='https://drive.google.com/file/d/1iVmNqUEzmPrHsimYqfncO-rxpt7T-ekX/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/195vUJdaeFqCqprm6to6s_NEfo6pacMhm/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0008-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Mosaic-based omnidirectional depth estimation for view synthesis</a></span>
    <br/>
    <i>Min-jung Shin, Minji Cho, Woojune Park, Kyeongbo Kong, Joonsoo Kim, Kug-jin Yun, Gwangsoon Lee, Suk-Ju Kang</i>
    <br/>
    <a href='https://drive.google.com/file/d/1o8lQ35W7DsVWQuCsjHmRwscPe3qHX2gx/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/16LQyXp6cMG5ttMCjOKUpUUxLScdQlhPU/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0009-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, and Program</a></span>
    <br/>
    <i>Tiange Luo, Honglak Lee, Justin Johnson</i>
    <br/>
    <a href='https://drive.google.com/file/d/16LQyXp6cMG5ttMCjOKUpUUxLScdQlhPU/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1drW8odKGHqiZ-5Hykh6f2TsHveF8buO_/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0010-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis</a></span>
    <br/>
    <i>Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan Kautz, Alexander Keller, Sameh Khamis, Thomas MÃ¼eller, Charles Loop, Nathan Morrica, Koki Nagano, Towaki Takikawa, Stan Birchfield</i>
    <br/>
    <a href='https://drive.google.com/file/d/1SzKp_SD4-vyabtuo5RxMro2P6uZlWDyl/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1fVA7aTR1XbtfTepEvPSc79Zt-5lupzMt/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0011-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Recovering Detail in 3D Shapes Using Disparity Maps</a></span>
    <br/>
    <i>Marissa Ramirez de Chanlatte, Matheus Gadelha, Thibault Groueix, Radomir Mech</i>
    <br/>
    <a href='https://drive.google.com/file/d/1tZKknBI4iTdBJ_9lhZIY8nESDVDDPJFu/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15Lf7ki5o4f3YA7n6PXgfzQG5zxkJ3VNF/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0012-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Share With Thy Neighbors: Single-View Reconstruction by Cross-Instance Consistency</a></span>
    <br/>
    <i>Tom Monnier, Matthew Fisher, Alexei A. Efros, Mathieu Aubry</i>
    <br/>
    <a href='https://drive.google.com/file/d/116Ou7tuDWk6oOPaw-ng4PCM9mMz84jn-/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0013-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Towards Generalising Neural Implicit Representations</a></span>
    <br/>
    <i>Theo W. Costain, Victor A. Prisacariu</i>
    <br/>
    <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
  </div>
</div>
-->

<div class="row" id="committee">
  <div class="col-xs-12">
    <h2>Workshop Chairs</h2>
    <ul class="list-unstyled">
    <li class="task-point"> ZHOU, Yu, Nankai University, China </li> 
    <li class="task-point"> ZENG, Gangyan, Nanjing University of Science and Technology, China </li> 
    <li class="task-point"> XIE, Hongtao, University of Science and Technology of China, China </li> 
    <li class="task-point"> YIN, Xu-Cheng, University of Science and Technology Beijing, China </li> 
    </ul>
  </div>
</div><br>





<div class="row">
  <div class="col-xs-12">
    <h2>Program Committee Members (Alphabetical Order)</h2>
    <ul class="list-unstyled">
    <li class="task-point"> CHEN, Zhineng, Fudan University, China </li> 
    <li class="task-point"> CHENG, Wentao, Nankai University, China </li> 
    <li class="task-point"> FANG, Shancheng, Beijing Yuanshi Technology Company, China </li> 
    <li class="task-point"> GAO, Liangcai, Peking University, China </li> 
    <li class="task-point"> LIAN, Zhouhui, Peking University, China</li> 
    <li class="task-point"> LIU, Juhua, Wuhan University, China</li> 
    <li class="task-point"> YANG, Chun, University of Science and Technology Beijing, China</li> 
    <li class="task-point"> WANG, Yaxing, Nankai University, China</li> 
    <li class="task-point"> WANG, Yuxin, University of Science and Technology of China, China</li> 
    </ul>
  </div>
</div><br>



<div class="row">
  <div class="col-xs-12">
    <h2>Short CV of the Workshop Chairs</h2>
  </div>
</div>
<br>
<br>


<div class="row">
  <div class="col-md-12">
    <a href="https://cc.nankai.edu.cn/2021/0323/c13619a548854/page.htm"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/zhouyu.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://cc.nankai.edu.cn/2021/0323/c13619a548854/page.htm">Prof. ZHOU, Yu.</a></b> Yu Zhou holds the BSc, MSc and PhD degrees in computer science from Harbin Institute of Technology. As a professor and a PhD supervisor in college of computer science, Nankai University, his research interests include computer vision and deep learning, with a special interest in visual text processing, detection, recognition and understanding. He served as AC, SPC, and PC members of CVPR, ICCV, ECCV, NeurIPS, ICDAR, and etc, and reviewers of TPAMI, TIP, and etc. He has published over 80 papers in peer-reviewed journals and conferences including CVPR, ICCV, NeurIPS, TMM, TNNLS, and etc, and the paper PIMNet has been selected as the best paper candidate in ACM MM 2021.
    </p>
  </div>
</div><br><br>



<div class="row">
  <div class="col-md-12">
    <a href="https://teacher.njust.edu.cn/wlkjaq/cgy/list.htm"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/zenggangyan.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://teacher.njust.edu.cn/wlkjaq/cgy/list.htm">Prof. ZENG, Gangyan.</a></b> Gangyan Zeng is with the School of Cyber Science and Engineering, Nanjing University of Science and Technology. She received the B.S. and Ph.D. degrees in information and communication engineering from the Communication University of China, in 2018 and 2023, respectively. From 2020 to 2023, she was a visiting student at the Institute of Information Engineering, Chinese Academy of Sciences. Her primary research interests include computer vision and multimodal intelligence, with a particular focus on the analysis and understanding of scene text. She has published over 10 papers in journals and conferences including ACM MM, AAAI, and Pattern Recognition. She has also received several rewards in the field of document analysis such as the second runner-up at the 2020 CVPR DocVQA Challenge.
    </p>
  </div>
</div><br>


<br>


<div class="row">
  <div class="col-md-12">
    <a href="https://faculty.ustc.edu.cn/xiehongtao/zh_CN/index.htm"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/xiehongtao.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://faculty.ustc.edu.cn/xiehongtao/zh_CN/index.htm">Prof. XIE, Hongtao.</a></b> Hongtao Xie is currently a Professor at the School of Information Science and Technology, University of Science and Technology of China. He is a recipient of the National Excellent Science Fund and is recognized as an outstanding member of the Youth Innovation Promotion Association of the Chinese Academy of Sciences. Additionally, he is a member of the Chinese Academy of Sciencesâ€™ Network Space Security Expert Group. He is engaged in research in the field of artificial intelligence and multimedia content security, including visual content recognition and inference, cross-model recognition of scene text and images, cross-modal content analysis and understanding, intelligent content generation, and security. He has published over 80 academic papers as the first or corresponding author in top international journals and conferences, including IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE-TPAMI), IEEE Transactions on Image Processing (IEEE-TIP), IEEE Transactions on Knowledge and Data Engineering (IEEE-TKDE), NeurIPS (NIPS), International Conference on Computer Vision (ICCV), and Conference on Computer Vision and Pattern Recognition (CVPR). Among these publications, four have been highly cited according to the Essential Science Indicators (ESI), three are considered hot-topic papers, and two have received the Best Paper Award at conferences.
    </p>
  </div>
</div><br>


<br>


<div class="row">
  <div class="col-md-12">
    <a href="https://scce.ustb.edu.cn/shiziduiwu/jiaoshixinxi/2018-04-12/62.html"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/yinxucheng.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://scce.ustb.edu.cn/shiziduiwu/jiaoshixinxi/2018-04-12/62.html">Prof. YIN, Xu-Cheng.</a></b> Xu-Cheng Yin is a full professor of Department of Computer Science and Technology, and the dean of School of Computer and Communication Engineering, University of Science and Technology Beijing, China. He received the B.Sc. and M.Sc. degrees in computer science from the University of Science and Technology Beijing, China, in 1999 and 2002, respectively, and the Ph.D. degree in pattern recognition and intelligent systems from the Institute of Automation, Chinese Academy of Sciences, in 2006. He was a visiting professor in the College of Information and Computer Sciences, University of Massachusetts Amherst, USA, for three times (Jan 2013 to Jan 2014, Jul 2014 to Aug 2014, and Jul 2016 to Sep 2016). His research interests include pattern recognition, document analysis and recognition, computer vision. He has published more than 100 academic papers (IEEE T-PAMI, IEEE T-IP, CVPR, ICDAR, etc.). From 2013 to 2019, his team had won the first place of a series of text detection and recognition competition tasks for 15 times in ICDAR Robust Reading Competition.
    </p>
  </div>
</div><br>


<br>


<!-- <div class="row text-center">
  <div class="col-xs-2">
    <a href="https://www.cs.sfu.ca/~haoz/">
      <img class="people-pic" src="{{ "/static/img/people/hao.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.sfu.ca/~haoz/">Hao (Richard) Zhang</a>
      <h6>Simon Fraser University & Amazon</h6>
    </div>
  </div>
  
  <div class="col-xs-2">
    <a href="https://xu-zhang-1987.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/xu.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://xu-zhang-1987.github.io/">Xu Zhang</a>
      <h6>Amazon</h6>
    </div>
  </div>
</div> -->


<!-- <hr>

<h2>Senior Organizers</h2>
<div class="row text-center">


  <div class="col-xs-2">
    <a href="https://www.linkedin.com/in/devernay">
      <img class="people-pic" src="{{ "/static/img/people/fred.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.linkedin.com/in/devernay">Frederic Devernay</a>
      <h6>Amazon</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.cs.sfu.ca/~haoz/">
      <img class="people-pic" src="{{ "/static/img/people/hao.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.sfu.ca/~haoz/">Hao (Richard) Zhang</a>
      <h6>Simon Fraser University & Amazon</h6>
    </div>
  </div>
</div> -->

<!-- <div class="row">
  <div class="col-xs-12">
    <h2>Prior workshops in this series</h2>
    <a href="iccv2023">ICCV 2023: 3D Vision and Modeling Challenges in eCommerce</a><br/>
  </div>
</div>

<br/>
<br/> -->

<!-- {% if page.acknowledgements %}
<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<a name="/acknowledgements"></a>
<div class="row">
  <div class="col-xs-12">
    <p>
      We thank <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> for the webpage format.
    </p>
  </div>
</div>
{% endif %} -->



<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div><br>


<div class="row">
  <div class="col-md-12">
    <a href="https://justusthies.github.io/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/justus-thies.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://justusthies.github.io/">BBBBB</a></b> Professor at TU Darmstadt. He is interested in marker-less motion capturing of facial performances, human bodies as well as general non-rigid objects. Besides capturing and reconstructing reality, he works on AI-based
      synthesis techniques that allow for photorealistic image and video synthesis.
    </p>
    <p>
      <b>Title: </b> A Survey on AAAAA
    </p>
    <p>
      <b>Abstract: </b> Emerging techniques in neural radiance fields enable photorealistic 3D scene synthesis from multi-view captures. This lecture will demonstrate our framework achieving 30ms latency for dynamic object reconstruction, integrating differentiable physics simulation with neural rendering pipelines. Key innovations include a hybrid representation combining implicit surfaces with parametric models for deformable objects.
    </p>
  </div>
</div><br>


<br>



<div class="row">
  <div class="col-xs-12">
    <h2>Reference</h2>
  </div>
</div>
<a name="/reference"></a>
<div class="row">
  <div class="col-xs-12">
    <p>
      [1] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, Furu Wei, TextDiffuser: Diffusion Models as Text Painters, NeurIPS, 2023.
    <p></p>  
      [2] Zhenhang Li, Yan Shu, Weichao Zeng, Dongbao Yang, Yu Zhou, First Creating Backgrounds Then Rendering Texts: A New Paradigm for Visual Text Blending, ECAI, 2024.
    <p></p>  
      [3] Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, Xuansong Xie, AnyText: Multilingual Visual Text Generation And Editing, ICLR, 2024.
    <p></p>  
      [4] Weichao Zeng, Yan Shu, Zhenhang Li, Dongbao Yang, Yu Zhou, TextCtrl: Diffusion-based Scene Text Editing with Prior Guidance Control, NeurIPS, 2024.
    <p></p>  
      [5] Yan Shu, Weichao Zeng, Zhenhang Li, Fangmin Zhao, Yu Zhou, Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual Text Processing, arXiv, 2024.
    <p></p>  
      [6] Yuzhe Zhang, Jiawei Zhang, Hao Li, Zhouxia Wang, Luwei Hou, Dongqing Zou, Liheng Bian, Diffusion-based Blind Text Image Super-Resolution, CVPR, 2024.
    <p></p>  
      [7] Xiaoming Li, Wangmeng Zuo, Chen Change Loy, Learning Generative Structure Prior for Blind Text Image Super-resolution, CVPR, 2023.
    <p></p>  
      [8] Yonghui Wang, Wengang Zhou, Zhenbo Lu, Houqiang Li, UDoc-GAN: Unpaired Document Illumination Correction with Background Light Prior, ACM MM, 2022.
    <p></p>  
      [9] Ling Zhang, Yinghao He, Qing Zhang, Zheng Liu, Xiaolong Zhang, Chunxia Xiao, Document Image Shadow Removal Guided by Color-Aware Background, CVPR, 2023.
    <p></p>  
      [10] Floor Verhoeven, Tanguy Magne, Olga Sorkine-Hornung, UVDoc: Neural Grid-based Document Unwarping, SIGGRAPH, 2023.
    <p></p>  
      [11] Pu Li, Weize Quan, Jianwei Guo, Dong-Ming Yan, Layout-aware Single-image Document Flattening, ACM TOG, 2023.
    </p>
  </div>
</div>


<!-- <div class="row">
  <div class="col-xs-12">
    <h2>Reference</h2>
  </div>
</div>
<a name="/reference"></a>
<div class="row">
  <div class="col-xs-12" style="line-height: 1.2;">
    <div style="margin-bottom: 8px;">[1] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, Furu Wei, TextDiffuser: Diffusion Models as Text Painters, NeurIPS, 2023.</div>
    <div style="margin-bottom: 8px;">[2] Zhenhang Li, Yan Shu, Weichao Zeng, Dongbao Yang, Yu Zhou, First Creating Backgrounds Then Rendering Texts: A New Paradigm for Visual Text Blending, ECAI, 2024.</div>
    <div style="margin-bottom: 8px;">[3] Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, Xuansong Xie, AnyText: Multilingual Visual Text Generation And Editing, ICLR, 2024.</div>
    <div style="margin-bottom: 8px;">[4] Weichao Zeng, Yan Shu, Zhenhang Li, Dongbao Yang, Yu Zhou, TextCtrl: Diffusion-based Scene Text Editing with Prior Guidance Control, NeurIPS, 2024.</div>
    <div style="margin-bottom: 8px;">[5] Yan Shu, Weichao Zeng, Zhenhang Li, Fangmin Zhao, Yu Zhou, Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual Text Processing, arXiv, 2024.</div>
    <div style="margin-bottom: 8px;">[6] Yuzhe Zhang, Jiawei Zhang, Hao Li, Zhouxia Wang, Luwei Hou, Dongqing Zou, Liheng Bian, Diffusion-based Blind Text Image Super-Resolution, CVPR, 2024.</div>
    <div style="margin-bottom: 8px;">[7] Xiaoming Li, Wangmeng Zuo, Chen Change Loy, Learning Generative Structure Prior for Blind Text Image Super-resolution, CVPR, 2023.</div>
    <div style="margin-bottom: 8px;">[8] Yonghui Wang, Wengang Zhou, Zhenbo Lu, Houqiang Li, UDoc-GAN: Unpaired Document Illumination Correction with Background Light Prior, ACM MM, 2022.</div>
    <div style="margin-bottom: 8px;">[9] Ling Zhang, Yinghao He, Qing Zhang, Zheng Liu, Xiaolong Zhang, Chunxia Xiao, Document Image Shadow Removal Guided by Color-Aware Background, CVPR, 2023.</div>
    <div style="margin-bottom: 8px;">[10] Floor Verhoeven, Tanguy Magne, Olga Sorkine-Hornung, UVDoc: Neural Grid-based Document Unwarping, SIGGRAPH, 2023.</div>
    <div style="margin-bottom: 0;">[11] Pu Li, Weize Quan, Jianwei Guo, Dong-Ming Yan, Layout-aware Single-image Document Flattening, ACM TOG, 2023.</div>
  </div>
</div> -->


<br>

