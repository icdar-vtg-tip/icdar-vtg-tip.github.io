---
layout: project
urltitle:  "Visual Text Generation and Text Image Processing"
title: "Visual Text Generation and Text Image Processing"
categories: icdar, workshop, computer vision, computer graphics, deep learning
permalink: /
favicon: /static/img/ico/favicon.png
bibtex: true
paper: true
acknowledgements: ""
description: "Website for the Workshop on Visual Text Generation and Text Image Processing at ICDAR 2025"
---

<br>
<div class="row">
  <div class="col-xs-12">
    <center><h2>Workshop on</h2></center>
    <center><h1>Visual Text Generation and Text Image Processing</h1></center>
    <center><h2>ICDAR 2025 Workshop</h2></center>
    <center><span style="font-weight:400;">September 16-21, 2025 @ Wuhan, Hubei, China</span></center>
    <center><span style="color:#e74c3c;font-weight:400;"></span></center>
    <br />
  </div>
</div>

<hr>

<!--<b>ðŸ“¢ Calling all researchers and enthusiasts! ðŸš€ Join our thrilling fine-grained 3D part labeling challenge built on the Amazon Berkeley Objects (ABO) Dataset: <a href="https://eval.ai/web/challenges/challenge-page/2027/overview" target="_blank">https://eval.ai/web/challenges/challenge-page/2027/overview</a>.</b>-->
<!-- <b>ðŸ“¢ Live Q&A on Slido: <a href="https://app.sli.do/event/iYUGKVgj6AVdjGhY5dzvAd" target="_blank">https://tinyurl.com/3DVeComm-slido</a> (use it to ask questions for presentations and panel discussion).</b> <br/>
<b>ðŸ“¢ Remote presentations on Zoom: <a href="https://sfu.zoom.us/j/82710628380?pwd=OQaJJGWXuRr7IYakyPd8k6Em6iUAeg.1" target="_blank">https://tinyurl.com/3DVeComm-zoom</a>.</b> -->


<!-- <b>Join live stream <a href="https://live.allintheloop.net/Agenda/ortra/ortraECCV2022/View_agenda/236653">here</a> (ECCV registration required).</b>

<b>Submit questions to the authors of the accepted papers: <a href="https://forms.gle/FFFVHVeTtcVkWg2n8">https://forms.gle/FFFVHVeTtcVkWg2n8</a>.</b>

<b>Submit questions for the closing panel discussion using this google form: <a href="https://forms.gle/XADQAVR8HNavtVRj6">https://forms.gle/XADQAVR8HNavtVRj6</a>.</b>


 <b>Please give us your feedback on how the workshop went using this Google form: <a href='https://forms.gle/utMpEnF4hmUcR19S7'>https://forms.gle/utMpEnF4hmUcR19S7</a>.</b> -->

<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      Visual text generation and text image preprocessing are two fundamental areas that play crucial roles in modern visual text analysis systems and directly impact the performance of downstream tasks such as OCR, information extraction, and visual text understanding.
      Visual text generation addresses the critical challenge of data scarcity by creating diverse, high-quality synthetic datasets [1, 2]. This not only reduces the cost and time of manual data collection but also enables the creation of comprehensive training sets that cover various visual text types and edge cases [3, 4]. Text image preprocessing [5], serving as the foundation of reliable visual text analysis pipelines, tackles real-world challenges such as low resolution [6, 7], uneven illumination [8, 9], and geometric distortions [10, 11]. These techniques are essential for handling various visual text conditions, from documents to scene images.
      The synergy of these topics to visual text analysis is becoming increasingly important: visual text generation techniques help create better training data, while advanced text image preprocessing techniques improve text quality in real conditions, facilitating model recognition. This workshop aims to bring together researchers and practitioners to work on these two topics and foster innovations in visual text analysis.
      <!-- This workshop aims to bring together researchers working on 3D computer vision and graphics for eCommerce, with a focus on the three topics: 
      (1) 3D shape/scene understanding and generation e.g. semantic segmentation, affordance and motion, multi-view reconstruction; 
      (2) Digital human and fashion e.g. virtual try-ons and personalized fashion recommendation;
      (3) Foundation-model-assisted reasoning e.g. shape/scene synthesis from texts, language grounding in 3D models and diffusion-based 3D generative models. We successfully hosted the <a href="/iccv2023/">the first 3DV in eCommerce workshop at ICCV 2023</a> which was very well-received and inspiring to the audience. In this second workshop, we are inviting a fully new line of speakers including 3 keynote presentations from academia and 3 talks from industry experts.  -->
     <!--This workshop aims to bring together researchers working on generative models of 3D shapes and scenes with researchers and practitioners who use these generative models in a variety of research areas. For our purposes, we define "generative model" to include methods that synthesize geometry unconditionally as well as from sensory inputs (e.g. images), language, or other high-level specifications. Vision tasks that can benefit from such models include scene classification and segmentation, 3D reconstruction, human activity recognition, robotic visual navigation, question answering, and more.-->
    </p>
  </div>
</div> <br>







<div class="row">
  <div class="col-xs-12">
    <h2>Schedule</h2>
    <p>All times in Beijing Time (UTC+08:00)</p>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr>
          <td>Time</td>
          <td>Events</td>
          <td></td>
        </tr>
        <tr>
          <td>13:50- 14:00</td>
          <td>Opening Remarks</td>
          <td></td>
        </tr>
        <tr>
          <td>14:00 â€“ 14:40</td>
          <td>Dr.A
          <br/>
          <i>Invited Talk 1 : AAAA</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>14:40-15:20</td>
          <td>Prof.B
          <br/>
          <i>Invited Talk 2 : BBB</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>15:20-16:00</td>
          <td>Contributed Talks (best/ runner-up paper talks)</td>
          <td></td>
        </tr>
        <tr>
          <td>16:00 â€“ 16:20</td>
          <td>Coffee break</td>
          <td></td>
        </tr>
        <tr>
          <td>16:20 â€“ 17:40</td>
          <td>Poster or oral Session
          </td>
          <td></td>
        </tr>
      </tbody>
    </table>
  </div>
</div>
<br>



<div class="row" id="call">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
    <p>Acceptable submission topics may include but are not limited to:</p>
  </div>
</div>

<!-- <div class="row">
  <div class="col-xs-12">
    <ol style="line-height: 1.4; margin-top: 15px;">
      <li style="margin-bottom: 8px;">GANs-based and Diffusion-based models for text image synthesis.</li>
      <li style="margin-bottom: 8px;">Layout-aware document image generation.</li>
      <li style="margin-bottom: 8px;">Real-synthetic domain gap analysis.</li>
      <li style="margin-bottom: 8px;">Text generation model benchmarking.</li>
      <li style="margin-bottom: 8px;">Image text removal, editing, style transfer.</li>
      <li style="margin-bottom: 8px;">Shadow, ink, and watermark removal of text image.</li>
      <li style="margin-bottom: 8px;">Illumination correction, deblurring, and binarization of text image.</li>
      <li style="margin-bottom: 8px;">Text image super-resolution.</li>
      <li style="margin-bottom: 8px;">Document image dewarping.</li>
      <li style="margin-bottom: 8px;">Text segmentation.</li>
      <li style="margin-bottom: 0;">Tampered text detection.</li>
    </ol>
  </div>
</div><br> -->

<div class="row">
  <div class="col-xs-12">
    <ul class="list-unstyled" style="line-height: 1.4; margin-top: 15px;">
      <li class="task-point">â–¸ GANs-based and Diffusion-based models for text image synthesis</li>
      <li class="task-point">â–¸ Layout-aware document image generation</li>
      <li class="task-point">â–¸ Real-synthetic domain gap analysis</li>
      <li class="task-point">â–¸ Text generation model benchmarking</li>
      <li class="task-point">â–¸ Image text removal, editing, style transfer</li>
      <li class="task-point">â–¸ Shadow, ink, and watermark removal of text image</li>
      <li class="task-point">â–¸ Illumination correction, deblurring, and binarization of text image</li>
      <li class="task-point">â–¸ Text image super-resolution</li>
      <li class="task-point">â–¸ Document image dewarping</li>
      <li class="task-point">â–¸ Text segmentation</li>
      <li class="task-point">â–¸ Tampered text detection</li>
    </ul>
  </div>
</div><br>

<style>
.task-point {
  margin-bottom: 10px;
  padding-left: 20px;
  position: relative;
}
.task-point:before {
  content: "â–¸";
  color: #2c7be5;
  position: absolute;
  left: 0;
  font-size: 1.1em;
}
</style>



<div class="row">
  <div class="col-xs-12">
    <h2>Submission</h2>
    <p>The workshop is open to original papers of theoretical or practical nature. Papers should be formatted to follow the instructions in ICDAR webpage https://www.icdar2025.com/home. Papers are limited to 15 pages (not including references). This workshop will follow a double-blind review process. Authors should not include their names and affiliations anywhere in the manuscript. Authors should also ensure that their identity is not revealed in directly by citing their previous work in the third person and omitting acknowledgments until the camera-ready version. Papers have to be submitted via the workshop's CMT submission page. The submission link will be announced soon.<br> At least one author of each accepted paper must register for the workshop, in order to present the paper. For further instructions, please refer to the ICDAR 2025 webpage.</p>
  </div>
</div><br>



<!--<br>
<div class="row" id="accepted">
  <div class="col-md-12">
    <h2>Accepted Papers</h2>
  </div>
</div>

<div class="row text-center">
  <div class="col-md-12">
    <hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0001-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>3D GAN Inversion for Controllable Portrait Image Animation</a></span>
    <br/>
    <i>Connor Z. Lin, David B. Lindell, Eric R. Chan, Gordon Wetzstein</i>
    <br/>
    <a href='https://drive.google.com/file/d/18qcR7WjImq_wRpfLf2aI7cRb62JyMppY/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1ZEHbONpIk8tGnCY3PWB3gpw-Cqf-_fLt/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1HkDUt1z7M_Bv5THurz8-wewoqqkg7HFo/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0002-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>3DLatNav: Navigating Generative Latent Spaces for Semantic-Aware 3D Object Manipulation</a></span>
    <br/>
    <i>Amaya Dharmasiri, Dinithi Dissanayake, Mohamed Afham, Isuru Dissanayake, Ranga Rodrigo, Kanchana Thilakarathna</i>
    <br/>
    <a href='https://drive.google.com/file/d/1ZnObAPLwCYvUCqyreMr7dWOlWEvcEL4W/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1rD_YGMNfUkVA_7-RJ6SX8y_rjPZYZlbU/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0003-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>3D Semantic Label Transfer and Matching in Human-Robot Collaboration</a></span>
    <br/>
    <i>Szilvia Szeier, BenjÃ¡min Baffy, GÃ¡bor Baranyi, Joul Skaf, LÃ¡szlÃ³ KopÃ¡csi, Daniel Sonntag, GÃ¡bor SÃ¶rÃ¶s, and AndrÃ¡s LÅ‘rincz</i>
    <br/>
    <a href='https://drive.google.com/file/d/1GD065M4qj2BhT6ujZEv_kMTFTmBYWL6C/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15GsMVDqnVBQnmg-bIifgY8gENf-xtAn0/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0004-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Generative Multiplane Images: Making a 2D GAN 3D-Aware</a></span>
    <br/>
    <i>Xiaoming Zhao, Fangchang Ma, David GÃ¼era, Zhile Ren, Alexander G. Schwing, Alex Colburn</i>
    <br/>
    <a href='https://drive.google.com/file/d/13OVLhihZ5xQEuY_tTu94fZTu_U7WXOpO/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1H4X3FnV2GLhdxc4jJQD45T1EweLb3lZC/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0005-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Intrinsic Neural Fields: Learning Functions on Manifolds</a></span>
    <br/>
    <i>Lukas Koestler, Daniel Grittner, Michael Moeller, Daniel Cremers, Zorah LÃ¤hner</i>
    <br/>
    <a href='https://drive.google.com/file/d/1gfRpZL1HzAkyAVqnD-bWjWTPSNtBdlhk/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/19g5Nq3YIg8KdEkG77QN0QQpORgVeHH39/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0006-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Learning Joint Surface Atlases</a></span>
    <br/>
    <i>Theo Deprelle, Thibault Groueix, Noam Aigerman, Vladimir G. Kim, Mathieu Aubry</i>
    <br/>
    <a href='https://drive.google.com/file/d/1udFGRnASw9iLDf7PyKMCr_-oZOkU9msR/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1xlEOAWprb1TDx54MNKCVA-lx3uWXDvBn/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0007-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Learning Neural Radiance Fields from Multi-View Geometry</a></span>
    <br/>
    <i>Marco Orsingher, Paolo Zani, Paolo Medici, Massimo Bertozzi</i>
    <br/>
    <a href='https://drive.google.com/file/d/1iVmNqUEzmPrHsimYqfncO-rxpt7T-ekX/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/195vUJdaeFqCqprm6to6s_NEfo6pacMhm/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0008-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Mosaic-based omnidirectional depth estimation for view synthesis</a></span>
    <br/>
    <i>Min-jung Shin, Minji Cho, Woojune Park, Kyeongbo Kong, Joonsoo Kim, Kug-jin Yun, Gwangsoon Lee, Suk-Ju Kang</i>
    <br/>
    <a href='https://drive.google.com/file/d/1o8lQ35W7DsVWQuCsjHmRwscPe3qHX2gx/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/16LQyXp6cMG5ttMCjOKUpUUxLScdQlhPU/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0009-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, and Program</a></span>
    <br/>
    <i>Tiange Luo, Honglak Lee, Justin Johnson</i>
    <br/>
    <a href='https://drive.google.com/file/d/16LQyXp6cMG5ttMCjOKUpUUxLScdQlhPU/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1drW8odKGHqiZ-5Hykh6f2TsHveF8buO_/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0010-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis</a></span>
    <br/>
    <i>Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan Kautz, Alexander Keller, Sameh Khamis, Thomas MÃ¼eller, Charles Loop, Nathan Morrica, Koki Nagano, Towaki Takikawa, Stan Birchfield</i>
    <br/>
    <a href='https://drive.google.com/file/d/1SzKp_SD4-vyabtuo5RxMro2P6uZlWDyl/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1fVA7aTR1XbtfTepEvPSc79Zt-5lupzMt/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0011-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Recovering Detail in 3D Shapes Using Disparity Maps</a></span>
    <br/>
    <i>Marissa Ramirez de Chanlatte, Matheus Gadelha, Thibault Groueix, Radomir Mech</i>
    <br/>
    <a href='https://drive.google.com/file/d/1tZKknBI4iTdBJ_9lhZIY8nESDVDDPJFu/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15Lf7ki5o4f3YA7n6PXgfzQG5zxkJ3VNF/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0012-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Share With Thy Neighbors: Single-View Reconstruction by Cross-Instance Consistency</a></span>
    <br/>
    <i>Tom Monnier, Matthew Fisher, Alexei A. Efros, Mathieu Aubry</i>
    <br/>
    <a href='https://drive.google.com/file/d/116Ou7tuDWk6oOPaw-ng4PCM9mMz84jn-/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0013-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Towards Generalising Neural Implicit Representations</a></span>
    <br/>
    <i>Theo W. Costain, Victor A. Prisacariu</i>
    <br/>
    <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
  </div>
</div>
-->

<div class="row" id="commitee">
  <div class="col-xs-12">
    <h2>Workshop Chairs</h2>
  </div>
</div>

<div class="row text-center">
  <div class="col-xs-2">
    <a href="https://kwang-ether.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/kai.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://kwang-ether.github.io/">Kai Wang</a>
      <h6>Amazon</h6>
      <h6>(Primary Contact)</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://yi-ming-qian.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/yiming.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://yi-ming-qian.github.io/">Yiming Qian</a>
      <h6>Amazon</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://fenggenyu.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/fenggen.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://fenggenyu.github.io/">Fenggen Yu</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://lorisbaz.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/loris.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://lorisbaz.github.io/">Loris Bazzani</a>
      <h6>Amazon</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://angelxuanchang.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/angel.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://angelxuanchang.github.io/">Angel Chang</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://zouchuhang.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/chuhang.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://zouchuhang.github.io/">Chuhang Zou</a>
      <h6>Amazon</h6>
    </div>
  </div>
</div>

<div class="row text-center">
  <div class="col-xs-2">
    <a href="https://dritchie.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/daniel.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://dritchie.github.io/">Daniel Ritchie</a>
      <h6>Brown University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://paschalidoud.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/Despoina.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://paschalidoud.github.io/">Despoina Paschalidou</a>
      <h6>Stanford University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://panchagil.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/pancha.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://panchagil.github.io/">Francisca Gil-Ureta</a>
      <h6>Amazon</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="http://pgehler-homepage.s3-website-us-east-1.amazonaws.com/">
      <img class="people-pic" src="{{ "/static/img/people/peter.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://pgehler-homepage.s3-website-us-east-1.amazonaws.com/">Peter Gehler</a>
      <h6>Zalando</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.linkedin.com/in/brian-jackson-8701a2a2/">
      <img class="people-pic" src="{{ "/static/img/people/brian.jfif" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.linkedin.com/in/brian-jackson-8701a2a2/">Brian Jackson</a>
      <h6>Amazon</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://ps.is.mpg.de/~jromero">
      <img class="people-pic" src="{{ "/static/img/people/javier.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ps.is.mpg.de/~jromero">Javier Romero</a>
      <h6>Meta</h6>
    </div>
  </div>
</div>
<div class="row text-center">
  <div class="col-xs-2">
    <a href="https://jianwang-cmu.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/jian.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://jianwang-cmu.github.io/">Jian Wang</a>
      <h6>Snap</h6>
    </div>
  </div>  

  <div class="col-xs-2">
    <a href="https://www.cs.sfu.ca/~haoz/">
      <img class="people-pic" src="{{ "/static/img/people/hao.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.sfu.ca/~haoz/">Hao (Richard) Zhang</a>
      <h6>Simon Fraser University & Amazon</h6>
    </div>
  </div>
  
  <div class="col-xs-2">
    <a href="https://xu-zhang-1987.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/xu.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://xu-zhang-1987.github.io/">Xu Zhang</a>
      <h6>Amazon</h6>
    </div>
  </div>
</div>


<!-- <hr>

<h2>Senior Organizers</h2>
<div class="row text-center">


  <div class="col-xs-2">
    <a href="https://www.linkedin.com/in/devernay">
      <img class="people-pic" src="{{ "/static/img/people/fred.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.linkedin.com/in/devernay">Frederic Devernay</a>
      <h6>Amazon</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.cs.sfu.ca/~haoz/">
      <img class="people-pic" src="{{ "/static/img/people/hao.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.sfu.ca/~haoz/">Hao (Richard) Zhang</a>
      <h6>Simon Fraser University & Amazon</h6>
    </div>
  </div>
</div> -->

<!-- <div class="row">
  <div class="col-xs-12">
    <h2>Prior workshops in this series</h2>
    <a href="iccv2023">ICCV 2023: 3D Vision and Modeling Challenges in eCommerce</a><br/>
  </div>
</div>

<br/>
<br/> -->

<!-- {% if page.acknowledgements %}
<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<a name="/acknowledgements"></a>
<div class="row">
  <div class="col-xs-12">
    <p>
      We thank <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> for the webpage format.
    </p>
  </div>
</div>
{% endif %} -->
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div><br>

<!-- <div class="row">
  <div class="col-md-12">
    <a href="https://people.mpi-inf.mpg.de/~theobalt/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/Christian_Theobalt_stehend.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://people.mpi-inf.mpg.de/~theobalt/">Christian Theobalt</a></b> Professor and the Director of the Visual Computing and AI Department at Max Planck Institute for Informatics. He works in computer vision and graphics, with a focus on 3D shape understanding. His long
      term vision to develop entirely new ways to capture, represent, synthesize and
      simulate models of the real world at highest detail, robustness, and efficiency.
    </p>
  </div>
</div><br> -->

<div class="row">
  <div class="col-md-12">
    <a href="https://people.eecs.berkeley.edu/~kanazawa/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/angjoo.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a></b> Assistant Professor at UC Berkeley. Her research focuses on the perception of the dynamic 3D world behind everyday photographs and video. Her lab is interested in developing methods that can learn a structured model of the world from visual observations. She also advises Luma AI.
    </p>
  </div>
</div><br>

<!-- <div class="row">
  <div class="col-md-12">
    <a href="https://www.irakemelmacher.com/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/Kemelmacher.jfif" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://www.irakemelmacher.com/">Ira Kemelmacher-Shlizerman</a></b> Professor at the University of Washington and a Principal Scientist at Google working on human modeling. Her team works
      on Generative AI imagery, and 3D experiences for Google online shopping, with
      a recent focus on apparel virtual try-on and 3D shoe spins.
    </p>
  </div>
</div><br> -->

<div class="row">
  <div class="col-md-12">
    <a href="https://justusthies.github.io/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/justus-thies.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://justusthies.github.io/">Justus Thies</a></b> Professor at TU Darmstadt. He is interested in marker-less motion capturing of facial performances, human bodies as well as general non-rigid objects. Besides capturing and reconstructing reality, he works on AI-based
      synthesis techniques that allow for photorealistic image and video synthesis.
    </p>
  </div>
</div><br>

<!-- <div class="row">
  <div class="col-md-12">
    <a href="https://www.elor.sites.tau.ac.il/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/Hadar2022_JPG.webp" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://www.elor.sites.tau.ac.il/">Hadar Averbuch-Elor</a></b> Assistant Professor at Tel-Aviv University. Her research interests are in computer graphics and vision, particularly combining
      pixels with more structured modalities, such as natural language and 3D geometry, for generating multimodal representations that are better suited for handling the full complexity of the visual world.
    </p>
  </div>
</div><br> -->

<div class="row">
  <div class="col-md-12">
    <a href="https://people.csail.mit.edu/ganchuang/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/Chuang_Gan.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a></b> Assistant Professor at UMass Amherst and a research manager at MIT-IBM Watson AI Lab. The overarching goal of his research is to
      build a human-like autonomous agents that is capable of sensing, reasoning, and
      acting in the physical world, with a recent focus on 3D large language models.
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="https://imatge.upc.edu/web/people/xavier-giro"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/xavier.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://imatge.upc.edu/web/people/xavier-giro">Xavier GirÃ³</a></b> Applied Scientist at Amazon Barcelona, in the team lead by Aleix Martinez and Francesc Moreno-Noguer. Before joining Amazon, he was an associate professor at the Universitat Politecnica de Catalunya (UPC), also in Barcelona. His current research interests focus on image generation, and their automatic quality assessment.
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="https://scholar.google.com/citations?user=Y9YnFoUAAAAJ&hl=en/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/reza.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://scholar.google.com/citations?user=Y9YnFoUAAAAJ&hl=en/">Reza Shirvany</a></b> Director of Applied Science at Zalando, leading a multi-disciplinary 
      team of applied scientists that develop customer facing AI driven
      products in online Fashion, for example the Zalando Virtual Fitting Room.
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="http://flycooler.com/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/zhao.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="http://flycooler.com/">Zhao Dong</a></b> Graphics Research Lead at Meta Reality Labs. He leads a team at Meta on computer graphics, aiming at building next-gen human centric computing platform for AR/VR/Metaverse.
    </p>
  </div>
</div><br>










<div class="row">
  <div class="col-xs-12">
    <h2>Reference</h2>
  </div>
</div>
<a name="/reference"></a>
<div class="row">
  <div class="col-xs-12">
    <p>
      [1] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, Furu Wei, TextDiffuser: Diffusion Models as Text Painters, NeurIPS, 2023.
    <p></p>  
      [2] Zhenhang Li, Yan Shu, Weichao Zeng, Dongbao Yang, Yu Zhou, First Creating Backgrounds Then Rendering Texts: A New Paradigm for Visual Text Blending, ECAI, 2024.
    <p></p>  
      [3] Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, Xuansong Xie, AnyText: Multilingual Visual Text Generation And Editing, ICLR, 2024.
    <p></p>  
      [4] Weichao Zeng, Yan Shu, Zhenhang Li, Dongbao Yang, Yu Zhou, TextCtrl: Diffusion-based Scene Text Editing with Prior Guidance Control, NeurIPS, 2024.
    <p></p>  
      [5] Yan Shu, Weichao Zeng, Zhenhang Li, Fangmin Zhao, Yu Zhou, Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual Text Processing, arXiv, 2024.
    <p></p>  
      [6] Yuzhe Zhang, Jiawei Zhang, Hao Li, Zhouxia Wang, Luwei Hou, Dongqing Zou, Liheng Bian, Diffusion-based Blind Text Image Super-Resolution, CVPR, 2024.
    <p></p>  
      [7] Xiaoming Li, Wangmeng Zuo, Chen Change Loy, Learning Generative Structure Prior for Blind Text Image Super-resolution, CVPR, 2023.
    <p></p>  
      [8] Yonghui Wang, Wengang Zhou, Zhenbo Lu, Houqiang Li, UDoc-GAN: Unpaired Document Illumination Correction with Background Light Prior, ACM MM, 2022.
    <p></p>  
      [9] Ling Zhang, Yinghao He, Qing Zhang, Zheng Liu, Xiaolong Zhang, Chunxia Xiao, Document Image Shadow Removal Guided by Color-Aware Background, CVPR, 2023.
    <p></p>  
      [10] Floor Verhoeven, Tanguy Magne, Olga Sorkine-Hornung, UVDoc: Neural Grid-based Document Unwarping, SIGGRAPH, 2023.
    <p></p>  
      [11] Pu Li, Weize Quan, Jianwei Guo, Dong-Ming Yan, Layout-aware Single-image Document Flattening, ACM TOG, 2023.
    </p>
  </div>
</div>


<!-- <div class="row">
  <div class="col-xs-12">
    <h2>Reference</h2>
  </div>
</div>
<a name="/reference"></a>
<div class="row">
  <div class="col-xs-12" style="line-height: 1.2;">
    <div style="margin-bottom: 8px;">[1] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, Furu Wei, TextDiffuser: Diffusion Models as Text Painters, NeurIPS, 2023.</div>
    <div style="margin-bottom: 8px;">[2] Zhenhang Li, Yan Shu, Weichao Zeng, Dongbao Yang, Yu Zhou, First Creating Backgrounds Then Rendering Texts: A New Paradigm for Visual Text Blending, ECAI, 2024.</div>
    <div style="margin-bottom: 8px;">[3] Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, Xuansong Xie, AnyText: Multilingual Visual Text Generation And Editing, ICLR, 2024.</div>
    <div style="margin-bottom: 8px;">[4] Weichao Zeng, Yan Shu, Zhenhang Li, Dongbao Yang, Yu Zhou, TextCtrl: Diffusion-based Scene Text Editing with Prior Guidance Control, NeurIPS, 2024.</div>
    <div style="margin-bottom: 8px;">[5] Yan Shu, Weichao Zeng, Zhenhang Li, Fangmin Zhao, Yu Zhou, Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual Text Processing, arXiv, 2024.</div>
    <div style="margin-bottom: 8px;">[6] Yuzhe Zhang, Jiawei Zhang, Hao Li, Zhouxia Wang, Luwei Hou, Dongqing Zou, Liheng Bian, Diffusion-based Blind Text Image Super-Resolution, CVPR, 2024.</div>
    <div style="margin-bottom: 8px;">[7] Xiaoming Li, Wangmeng Zuo, Chen Change Loy, Learning Generative Structure Prior for Blind Text Image Super-resolution, CVPR, 2023.</div>
    <div style="margin-bottom: 8px;">[8] Yonghui Wang, Wengang Zhou, Zhenbo Lu, Houqiang Li, UDoc-GAN: Unpaired Document Illumination Correction with Background Light Prior, ACM MM, 2022.</div>
    <div style="margin-bottom: 8px;">[9] Ling Zhang, Yinghao He, Qing Zhang, Zheng Liu, Xiaolong Zhang, Chunxia Xiao, Document Image Shadow Removal Guided by Color-Aware Background, CVPR, 2023.</div>
    <div style="margin-bottom: 8px;">[10] Floor Verhoeven, Tanguy Magne, Olga Sorkine-Hornung, UVDoc: Neural Grid-based Document Unwarping, SIGGRAPH, 2023.</div>
    <div style="margin-bottom: 0;">[11] Pu Li, Weize Quan, Jianwei Guo, Dong-Ming Yan, Layout-aware Single-image Document Flattening, ACM TOG, 2023.</div>
  </div>
</div> -->


<br>

