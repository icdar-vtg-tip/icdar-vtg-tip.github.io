<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords" content="eccv, workshop, computer vision, computer graphics, fashion, 3D, reconstruction, modeling, natural language processing, human">

  <link rel="shortcut icon" href="/static/img/ico/favicon.png">



  <title>3D Vision and Modeling Challenges in eCommerce</title>
  <meta name="description" content="Website for the Workshop on 3D Vision and Modeling Challenges in eCommerce at ECCV 2024">

  <!--Open Graph Related Stuff-->
  <meta property="og:title" content="3D Vision and Modeling Challenges in eCommerce"/>
  <meta property="og:url" content="https://3dv-in-ecommerce.github.io/"/>
  <meta property="og:description" content="Website for the Workshop on 3D Vision and Modeling Challenges in eCommerce at ECCV 2024"/>
  <meta property="og:site_name" content="3D Vision and Modeling Challenges in eCommerce"/>
  <meta property="og:image" content=""/>
  <meta property="og:image:url" content=""/>

  <!--Twitter Card Stuff-->
  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:title" content="3D Vision and Modeling Challenges in eCommerce"/>
  <meta name="twitter:image" content="https://3dv-in-ecommerce.github.io/static/img/bg.png">
  <meta name="twitter:url" content="https://3dv-in-ecommerce.github.io/"/>
  <meta name="twitter:description" content="Website for the Workshop on 3D Vision and Modeling Challenges in eCommerce at ECCV 2024"/>

  <!-- CSS  -->
  <link rel="stylesheet" type="text/css" href="/static/css/bootstrap.min.css">
  <link rel="stylesheet" type="text/css" href="/static/css/main.css" media="screen,projection">
</head>

  <body>

    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    
    <div class="navbar-header">
      <a class="navbar-brand" href="/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
        <li><a href="#intro">Introduction</a></li>
        <!-- <li><a href="#cfp">Challenge</a></li> -->
        <!-- <li><a href="#dates">Important Dates</a></li> -->
        <li><a href="#schedule">Schedule</a></li>
        <!--<li><a href="#accepted">Accepted Papers</a></li>-->
        <li><a href="#speakers">Invited Speakers</a></li>
        <li><a href="#organizers">Organizers</a></li>
      </ul>
    </div>

  </div>
</div>


    <div class="container">
      <div class="page-content">
          <p><br /></p>
<div class="row">
  <div class="col-xs-12">
    <center><h2>Second Workshop on</h2></center>
    <center><h1>3D Vision and Modeling Challenges in eCommerce</h1></center>
    <center><h2>ECCV 2024 Workshop</h2></center>
    <center><span style="font-weight:400;">Sep 29, 2024 @ Milan, Italy</span></center>
    <center><span style="font-weight:400;">Room Brown 2, MiCo Milano Convention Center</span></center>
    <center><span style="color:#e74c3c;font-weight:400;"></span></center>
    <br />
  </div>
</div>

<hr />

<!--<b>📢 Calling all researchers and enthusiasts! 🚀 Join our thrilling fine-grained 3D part labeling challenge built on the Amazon Berkeley Objects (ABO) Dataset: <a href="https://eval.ai/web/challenges/challenge-page/2027/overview" target="_blank">https://eval.ai/web/challenges/challenge-page/2027/overview</a>.</b>-->
<!-- <b>📢 Live Q&A on Slido: <a href="https://app.sli.do/event/iYUGKVgj6AVdjGhY5dzvAd" target="_blank">https://tinyurl.com/3DVeComm-slido</a> (use it to ask questions for presentations and panel discussion).</b> <br/>
<b>📢 Remote presentations on Zoom: <a href="https://sfu.zoom.us/j/82710628380?pwd=OQaJJGWXuRr7IYakyPd8k6Em6iUAeg.1" target="_blank">https://tinyurl.com/3DVeComm-zoom</a>.</b> -->

<!-- <b>Join live stream <a href="https://live.allintheloop.net/Agenda/ortra/ortraECCV2022/View_agenda/236653">here</a> (ECCV registration required).</b>

<b>Submit questions to the authors of the accepted papers: <a href="https://forms.gle/FFFVHVeTtcVkWg2n8">https://forms.gle/FFFVHVeTtcVkWg2n8</a>.</b>

<b>Submit questions for the closing panel discussion using this google form: <a href="https://forms.gle/XADQAVR8HNavtVRj6">https://forms.gle/XADQAVR8HNavtVRj6</a>.</b>


 <b>Please give us your feedback on how the workshop went using this Google form: <a href='https://forms.gle/utMpEnF4hmUcR19S7'>https://forms.gle/utMpEnF4hmUcR19S7</a>.</b> -->

<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      This workshop aims to bring together researchers working on 3D computer vision and graphics for eCommerce, with a focus on the three topics: 
      (1) 3D shape/scene understanding and generation e.g. semantic segmentation, affordance and motion, multi-view reconstruction; 
      (2) Digital human and fashion e.g. virtual try-ons and personalized fashion recommendation;
      (3) Foundation-model-assisted reasoning e.g. shape/scene synthesis from texts, language grounding in 3D models and diffusion-based 3D generative models. We successfully hosted the <a href="/iccv2023/">the first 3DV in eCommerce workshop at ICCV 2023</a> which was very well-received and inspiring to the audience. In this second workshop, we are inviting a fully new line of speakers including 3 keynote presentations from academia and 3 talks from industry experts. 
     <!--This workshop aims to bring together researchers working on generative models of 3D shapes and scenes with researchers and practitioners who use these generative models in a variety of research areas. For our purposes, we define "generative model" to include methods that synthesize geometry unconditionally as well as from sensory inputs (e.g. images), language, or other high-level specifications. Vision tasks that can benefit from such models include scene classification and segmentation, 3D reconstruction, human activity recognition, robotic visual navigation, question answering, and more.-->
    </p>
  </div>
</div>
<p><br /></p>

<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Schedule</h2>
    <p>All times in Italy Time (UTC+02:00)</p>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr>
          <td>9:00am - 9:05am</td>
          <td>Welcome and introduction</td>
          <td></td>
        </tr>
        <tr>
          <td>9:05am - 9:35am</td>
          <td>Justus Thies
          <br />
          <i>Title: Multi-modal 3D Human Analysis and Synthesis</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>9:35am - 10:05am</td>
          <td>Zhao Dong
          <br />
          <i>Title: Democratizing Digital Twins: Path to Frictionless 3D Digital Twin Creation for Everyone</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>10:05am - 10:35am</td>
          <td>Xavier Giró
          <br />
          <i>Title: Image Generation at Scale</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>10:35am - 11:00am</td>
          <td>Coffee break</td>
          <td></td>
        </tr>
        <tr>
          <td>11:00am - 11:30am</td>
          <td>Angjoo Kanazawa
          <br />
          <i>Title: Scaling 3D Capture and Latest Updates on nerfstudio/gsplat</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>11:30am - 12:00pm</td>
          <td>Reza Shirvany
          <br />
          <i>Title: Reshaping Fashion: 3D and Generative AI Solutions for Perfect Size and Fit</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>12:00pm - 12:30pm</td>
          <td>Chuang Gan
          <br />
          <i>Title: Building 3D World Models for Embodied General Intelligence</i>
          </td>
          <td></td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div>
<p><br /></p>

<!-- <div class="row">
  <div class="col-md-12">
    <a href="https://people.mpi-inf.mpg.de/~theobalt/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/Christian_Theobalt_stehend.jpg"></a>
    <p>
      <b><a href="https://people.mpi-inf.mpg.de/~theobalt/">Christian Theobalt</a></b> Professor and the Director of the Visual Computing and AI Department at Max Planck Institute for Informatics. He works in computer vision and graphics, with a focus on 3D shape understanding. His long
      term vision to develop entirely new ways to capture, represent, synthesize and
      simulate models of the real world at highest detail, robustness, and efficiency.
    </p>
  </div>
</div><br> -->

<div class="row">
  <div class="col-md-12">
    <a href="https://people.eecs.berkeley.edu/~kanazawa/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/angjoo.jpg" /></a>
    <p>
      <b><a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a></b> Assistant Professor at UC Berkeley. Her research focuses on the perception of the dynamic 3D world behind everyday photographs and video. Her lab is interested in developing methods that can learn a structured model of the world from visual observations. She also advises Luma AI.
    </p>
  </div>
</div>
<p><br /></p>

<!-- <div class="row">
  <div class="col-md-12">
    <a href="https://www.irakemelmacher.com/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/Kemelmacher.jfif"></a>
    <p>
      <b><a href="https://www.irakemelmacher.com/">Ira Kemelmacher-Shlizerman</a></b> Professor at the University of Washington and a Principal Scientist at Google working on human modeling. Her team works
      on Generative AI imagery, and 3D experiences for Google online shopping, with
      a recent focus on apparel virtual try-on and 3D shoe spins.
    </p>
  </div>
</div><br> -->

<div class="row">
  <div class="col-md-12">
    <a href="https://justusthies.github.io/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/justus-thies.jpg" /></a>
    <p>
      <b><a href="https://justusthies.github.io/">Justus Thies</a></b> Professor at TU Darmstadt. He is interested in marker-less motion capturing of facial performances, human bodies as well as general non-rigid objects. Besides capturing and reconstructing reality, he works on AI-based
      synthesis techniques that allow for photorealistic image and video synthesis.
    </p>
  </div>
</div>
<p><br /></p>

<!-- <div class="row">
  <div class="col-md-12">
    <a href="https://www.elor.sites.tau.ac.il/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/Hadar2022_JPG.webp"></a>
    <p>
      <b><a href="https://www.elor.sites.tau.ac.il/">Hadar Averbuch-Elor</a></b> Assistant Professor at Tel-Aviv University. Her research interests are in computer graphics and vision, particularly combining
      pixels with more structured modalities, such as natural language and 3D geometry, for generating multimodal representations that are better suited for handling the full complexity of the visual world.
    </p>
  </div>
</div><br> -->

<div class="row">
  <div class="col-md-12">
    <a href="https://people.csail.mit.edu/ganchuang/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/Chuang_Gan.jpg" /></a>
    <p>
      <b><a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a></b> Assistant Professor at UMass Amherst and a research manager at MIT-IBM Watson AI Lab. The overarching goal of his research is to
      build a human-like autonomous agents that is capable of sensing, reasoning, and
      acting in the physical world, with a recent focus on 3D large language models.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://imatge.upc.edu/web/people/xavier-giro"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/xavier.jpg" /></a>
    <p>
      <b><a href="https://imatge.upc.edu/web/people/xavier-giro">Xavier Giró</a></b> Applied Scientist at Amazon Barcelona, in the team lead by Aleix Martinez and Francesc Moreno-Noguer. Before joining Amazon, he was an associate professor at the Universitat Politecnica de Catalunya (UPC), also in Barcelona. His current research interests focus on image generation, and their automatic quality assessment.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://scholar.google.com/citations?user=Y9YnFoUAAAAJ&amp;hl=en/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/reza.jpg" /></a>
    <p>
      <b><a href="https://scholar.google.com/citations?user=Y9YnFoUAAAAJ&amp;hl=en/">Reza Shirvany</a></b> Director of Applied Science at Zalando, leading a multi-disciplinary 
      team of applied scientists that develop customer facing AI driven
      products in online Fashion, for example the Zalando Virtual Fitting Room.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="http://flycooler.com/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/zhao.jpg" /></a>
    <p>
      <b><a href="http://flycooler.com/">Zhao Dong</a></b> Graphics Research Lead at Meta Reality Labs. He leads a team at Meta on computer graphics, aiming at building next-gen human centric computing platform for AR/VR/Metaverse.
    </p>
  </div>
</div>
<p><br /></p>

<!--<br>
<div class="row" id="accepted">
  <div class="col-md-12">
    <h2>Accepted Papers</h2>
  </div>
</div>

<div class="row text-center">
  <div class="col-md-12">
    <hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0001-poster.png" width='700'><br/>
    <a href=''>3D GAN Inversion for Controllable Portrait Image Animation</a></span>
    <br/>
    <i>Connor Z. Lin, David B. Lindell, Eric R. Chan, Gordon Wetzstein</i>
    <br/>
    <a href='https://drive.google.com/file/d/18qcR7WjImq_wRpfLf2aI7cRb62JyMppY/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1ZEHbONpIk8tGnCY3PWB3gpw-Cqf-_fLt/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1HkDUt1z7M_Bv5THurz8-wewoqqkg7HFo/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0002-poster.png" width='700'><br/>
    <a href=''>3DLatNav: Navigating Generative Latent Spaces for Semantic-Aware 3D Object Manipulation</a></span>
    <br/>
    <i>Amaya Dharmasiri, Dinithi Dissanayake, Mohamed Afham, Isuru Dissanayake, Ranga Rodrigo, Kanchana Thilakarathna</i>
    <br/>
    <a href='https://drive.google.com/file/d/1ZnObAPLwCYvUCqyreMr7dWOlWEvcEL4W/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1rD_YGMNfUkVA_7-RJ6SX8y_rjPZYZlbU/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0003-poster.png" width='700'><br/>
    <a href=''>3D Semantic Label Transfer and Matching in Human-Robot Collaboration</a></span>
    <br/>
    <i>Szilvia Szeier, Benjámin Baffy, Gábor Baranyi, Joul Skaf, László Kopácsi, Daniel Sonntag, Gábor Sörös, and András Lőrincz</i>
    <br/>
    <a href='https://drive.google.com/file/d/1GD065M4qj2BhT6ujZEv_kMTFTmBYWL6C/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15GsMVDqnVBQnmg-bIifgY8gENf-xtAn0/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0004-poster.png" width='700'><br/>
    <a href=''>Generative Multiplane Images: Making a 2D GAN 3D-Aware</a></span>
    <br/>
    <i>Xiaoming Zhao, Fangchang Ma, David Güera, Zhile Ren, Alexander G. Schwing, Alex Colburn</i>
    <br/>
    <a href='https://drive.google.com/file/d/13OVLhihZ5xQEuY_tTu94fZTu_U7WXOpO/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1H4X3FnV2GLhdxc4jJQD45T1EweLb3lZC/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0005-poster.png" width='700'><br/>
    <a href=''>Intrinsic Neural Fields: Learning Functions on Manifolds</a></span>
    <br/>
    <i>Lukas Koestler, Daniel Grittner, Michael Moeller, Daniel Cremers, Zorah Lähner</i>
    <br/>
    <a href='https://drive.google.com/file/d/1gfRpZL1HzAkyAVqnD-bWjWTPSNtBdlhk/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/19g5Nq3YIg8KdEkG77QN0QQpORgVeHH39/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0006-poster.png" width='700'><br/>
    <a href=''>Learning Joint Surface Atlases</a></span>
    <br/>
    <i>Theo Deprelle, Thibault Groueix, Noam Aigerman, Vladimir G. Kim, Mathieu Aubry</i>
    <br/>
    <a href='https://drive.google.com/file/d/1udFGRnASw9iLDf7PyKMCr_-oZOkU9msR/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1xlEOAWprb1TDx54MNKCVA-lx3uWXDvBn/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0007-poster.png" width='700'><br/>
    <a href=''>Learning Neural Radiance Fields from Multi-View Geometry</a></span>
    <br/>
    <i>Marco Orsingher, Paolo Zani, Paolo Medici, Massimo Bertozzi</i>
    <br/>
    <a href='https://drive.google.com/file/d/1iVmNqUEzmPrHsimYqfncO-rxpt7T-ekX/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/195vUJdaeFqCqprm6to6s_NEfo6pacMhm/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0008-poster.png" width='700'><br/>
    <a href=''>Mosaic-based omnidirectional depth estimation for view synthesis</a></span>
    <br/>
    <i>Min-jung Shin, Minji Cho, Woojune Park, Kyeongbo Kong, Joonsoo Kim, Kug-jin Yun, Gwangsoon Lee, Suk-Ju Kang</i>
    <br/>
    <a href='https://drive.google.com/file/d/1o8lQ35W7DsVWQuCsjHmRwscPe3qHX2gx/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/16LQyXp6cMG5ttMCjOKUpUUxLScdQlhPU/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0009-poster.png" width='700'><br/>
    <a href=''>Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, and Program</a></span>
    <br/>
    <i>Tiange Luo, Honglak Lee, Justin Johnson</i>
    <br/>
    <a href='https://drive.google.com/file/d/16LQyXp6cMG5ttMCjOKUpUUxLScdQlhPU/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1drW8odKGHqiZ-5Hykh6f2TsHveF8buO_/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0010-poster.png" width='700'><br/>
    <a href=''>RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis</a></span>
    <br/>
    <i>Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan Kautz, Alexander Keller, Sameh Khamis, Thomas Müeller, Charles Loop, Nathan Morrica, Koki Nagano, Towaki Takikawa, Stan Birchfield</i>
    <br/>
    <a href='https://drive.google.com/file/d/1SzKp_SD4-vyabtuo5RxMro2P6uZlWDyl/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1fVA7aTR1XbtfTepEvPSc79Zt-5lupzMt/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0011-poster.png" width='700'><br/>
    <a href=''>Recovering Detail in 3D Shapes Using Disparity Maps</a></span>
    <br/>
    <i>Marissa Ramirez de Chanlatte, Matheus Gadelha, Thibault Groueix, Radomir Mech</i>
    <br/>
    <a href='https://drive.google.com/file/d/1tZKknBI4iTdBJ_9lhZIY8nESDVDDPJFu/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15Lf7ki5o4f3YA7n6PXgfzQG5zxkJ3VNF/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0012-poster.png" width='700'><br/>
    <a href=''>Share With Thy Neighbors: Single-View Reconstruction by Cross-Instance Consistency</a></span>
    <br/>
    <i>Tom Monnier, Matthew Fisher, Alexei A. Efros, Mathieu Aubry</i>
    <br/>
    <a href='https://drive.google.com/file/d/116Ou7tuDWk6oOPaw-ng4PCM9mMz84jn-/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0013-poster.png" width='700'><br/>
    <a href=''>Towards Generalising Neural Implicit Representations</a></span>
    <br/>
    <i>Theo W. Costain, Victor A. Prisacariu</i>
    <br/>
    <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
  </div>
</div>
-->

<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row text-center">
  <div class="col-xs-2">
    <a href="https://kwang-ether.github.io/">
      <img class="people-pic" src="/static/img/people/kai.jpg" />
    </a>
    <div class="people-name">
      <a href="https://kwang-ether.github.io/">Kai Wang</a>
      <h6>Amazon</h6>
      <h6>(Primary Contact)</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://yi-ming-qian.github.io/">
      <img class="people-pic" src="/static/img/people/yiming.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://yi-ming-qian.github.io/">Yiming Qian</a>
      <h6>Amazon</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://fenggenyu.github.io/">
      <img class="people-pic" src="/static/img/people/fenggen.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://fenggenyu.github.io/">Fenggen Yu</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://lorisbaz.github.io/">
      <img class="people-pic" src="/static/img/people/loris.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://lorisbaz.github.io/">Loris Bazzani</a>
      <h6>Amazon</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://angelxuanchang.github.io/">
      <img class="people-pic" src="/static/img/people/angel.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://angelxuanchang.github.io/">Angel Chang</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://zouchuhang.github.io/">
      <img class="people-pic" src="/static/img/people/chuhang.jpg" />
    </a>
    <div class="people-name">
      <a href="https://zouchuhang.github.io/">Chuhang Zou</a>
      <h6>Amazon</h6>
    </div>
  </div>
</div>

<div class="row text-center">
  <div class="col-xs-2">
    <a href="https://dritchie.github.io/">
      <img class="people-pic" src="/static/img/people/daniel.png" />
    </a>
    <div class="people-name">
      <a href="https://dritchie.github.io/">Daniel Ritchie</a>
      <h6>Brown University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://paschalidoud.github.io/">
      <img class="people-pic" src="/static/img/people/Despoina.jpg" />
    </a>
    <div class="people-name">
      <a href="https://paschalidoud.github.io/">Despoina Paschalidou</a>
      <h6>Stanford University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://panchagil.github.io/">
      <img class="people-pic" src="/static/img/people/pancha.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://panchagil.github.io/">Francisca Gil-Ureta</a>
      <h6>Amazon</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="http://pgehler-homepage.s3-website-us-east-1.amazonaws.com/">
      <img class="people-pic" src="/static/img/people/peter.jpg" />
    </a>
    <div class="people-name">
      <a href="http://pgehler-homepage.s3-website-us-east-1.amazonaws.com/">Peter Gehler</a>
      <h6>Zalando</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.linkedin.com/in/brian-jackson-8701a2a2/">
      <img class="people-pic" src="/static/img/people/brian.jfif" />
    </a>
    <div class="people-name">
      <a href="https://www.linkedin.com/in/brian-jackson-8701a2a2/">Brian Jackson</a>
      <h6>Amazon</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://ps.is.mpg.de/~jromero">
      <img class="people-pic" src="/static/img/people/javier.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://ps.is.mpg.de/~jromero">Javier Romero</a>
      <h6>Meta</h6>
    </div>
  </div>
</div>
<div class="row text-center">
  <div class="col-xs-2">
    <a href="https://jianwang-cmu.github.io/">
      <img class="people-pic" src="/static/img/people/jian.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://jianwang-cmu.github.io/">Jian Wang</a>
      <h6>Snap</h6>
    </div>
  </div>  

  <div class="col-xs-2">
    <a href="https://www.cs.sfu.ca/~haoz/">
      <img class="people-pic" src="/static/img/people/hao.jpg" />
    </a>
    <div class="people-name">
      <a href="https://www.cs.sfu.ca/~haoz/">Hao (Richard) Zhang</a>
      <h6>Simon Fraser University &amp; Amazon</h6>
    </div>
  </div>
  
  <div class="col-xs-2">
    <a href="https://xu-zhang-1987.github.io/">
      <img class="people-pic" src="/static/img/people/xu.jpg" />
    </a>
    <div class="people-name">
      <a href="https://xu-zhang-1987.github.io/">Xu Zhang</a>
      <h6>Amazon</h6>
    </div>
  </div>
</div>

<!-- <hr>

<h2>Senior Organizers</h2>
<div class="row text-center">


  <div class="col-xs-2">
    <a href="https://www.linkedin.com/in/devernay">
      <img class="people-pic" src="/static/img/people/fred.jpeg">
    </a>
    <div class="people-name">
      <a href="https://www.linkedin.com/in/devernay">Frederic Devernay</a>
      <h6>Amazon</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.cs.sfu.ca/~haoz/">
      <img class="people-pic" src="/static/img/people/hao.jpg">
    </a>
    <div class="people-name">
      <a href="https://www.cs.sfu.ca/~haoz/">Hao (Richard) Zhang</a>
      <h6>Simon Fraser University & Amazon</h6>
    </div>
  </div>
</div> -->

<div class="row">
  <div class="col-xs-12">
    <h2>Prior workshops in this series</h2>
    <a href="iccv2023">ICCV 2023: 3D Vision and Modeling Challenges in eCommerce</a><br />
  </div>
</div>

<p><br />
<br /></p>

<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<p><a name="/acknowledgements"></a></p>
<div class="row">
  <div class="col-xs-12">
    <p>
      We thank <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> for the webpage format.
    </p>
  </div>
</div>

<p><br /></p>


      </div>
    </div>

    

    <script type="text/javascript" src="/static/js/jquery.min.js"></script>
    <script type="text/javascript" src="/static/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="/static/js/main.js"></script>
  </body>
</html>
