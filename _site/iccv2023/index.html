<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords" content="eccv, workshop, computer vision, computer graphics, fashion, 3D, reconstruction, modeling, natural language processing, human">

  <link rel="shortcut icon" href="/static/img/ico/favicon.png">



  <title>3D Vision and Modeling Challenges in eCommerce at ICCV 2023</title>
  <meta name="description" content="Website for the Workshop on 3D Vision and Modeling Challenges in eCommerce at ICCV 2023">

  <!--Open Graph Related Stuff-->
  <meta property="og:title" content="3D Vision and Modeling Challenges in eCommerce"/>
  <meta property="og:url" content="https://3dv-in-ecommerce.github.io/"/>
  <meta property="og:description" content="Website for the Workshop on 3D Vision and Modeling Challenges in eCommerce at ICCV 2023"/>
  <meta property="og:site_name" content="3D Vision and Modeling Challenges in eCommerce"/>
  <meta property="og:image" content=""/>
  <meta property="og:image:url" content=""/>

  <!--Twitter Card Stuff-->
  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:title" content="3D Vision and Modeling Challenges in eCommerce"/>
  <meta name="twitter:image" content="https://3dv-in-ecommerce.github.io/static/img/bg.png">
  <meta name="twitter:url" content="https://3dv-in-ecommerce.github.io/"/>
  <meta name="twitter:description" content="Website for the Workshop on 3D Vision and Modeling Challenges in eCommerce at ICCV 2023"/>

  <!-- CSS  -->
  <link rel="stylesheet" type="text/css" href="/static/css/bootstrap.min.css">
  <link rel="stylesheet" type="text/css" href="/static/css/main.css" media="screen,projection">
</head>

  <body>

    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    
    <div class="navbar-header">
      <a class="navbar-brand" href="/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
        <li><a href="#intro">Introduction</a></li>
        <li><a href="#cfp">Challenge</a></li>
        <li><a href="#dates">Important Dates</a></li>
        <li><a href="#schedule">Schedule</a></li>
        <!--<li><a href="#accepted">Accepted Papers</a></li>-->
        <li><a href="#speakers">Invited Speakers</a></li>
        <li><a href="#organizers">Organizers</a></li>
      </ul>
    </div>

  </div>
</div>


    <div class="container">
      <div class="page-content">
          <p><br /></p>
<div class="row">
  <div class="col-xs-12">
    <center><h1>3D Vision and Modeling Challenges in eCommerce</h1></center>
    <center><h2>ICCV 2023 Workshop</h2></center>
    <center><span style="font-weight:400;">October 2, 2023 @ Paris, France</span></center>
    <center><span style="font-weight:400;">Room E06, Paris Convention Center</span></center>
    <center><span style="color:#e74c3c;font-weight:400;"></span></center>
    <br />
  </div>
</div>

<hr />

<!--<b>ðŸ“¢ Calling all researchers and enthusiasts! ðŸš€ Join our thrilling fine-grained 3D part labeling challenge built on the Amazon Berkeley Objects (ABO) Dataset: <a href="https://eval.ai/web/challenges/challenge-page/2027/overview" target="_blank">https://eval.ai/web/challenges/challenge-page/2027/overview</a>.</b>-->
<p><b>ðŸ“¢ Live Q&amp;A on Slido: <a href="https://app.sli.do/event/iYUGKVgj6AVdjGhY5dzvAd" target="_blank">https://tinyurl.com/3DVeComm-slido</a> (use it to ask questions for presentations and panel discussion).</b> <br />
<b>ðŸ“¢ Remote presentations on Zoom: <a href="https://sfu.zoom.us/j/82710628380?pwd=OQaJJGWXuRr7IYakyPd8k6Em6iUAeg.1" target="_blank">https://tinyurl.com/3DVeComm-zoom</a>.</b></p>

<!-- <b>Join live stream <a href="https://live.allintheloop.net/Agenda/ortra/ortraECCV2022/View_agenda/236653">here</a> (ECCV registration required).</b>

<b>Submit questions to the authors of the accepted papers: <a href="https://forms.gle/FFFVHVeTtcVkWg2n8">https://forms.gle/FFFVHVeTtcVkWg2n8</a>.</b>

<b>Submit questions for the closing panel discussion using this google form: <a href="https://forms.gle/XADQAVR8HNavtVRj6">https://forms.gle/XADQAVR8HNavtVRj6</a>.</b>


 <b>Please give us your feedback on how the workshop went using this Google form: <a href='https://forms.gle/utMpEnF4hmUcR19S7'>https://forms.gle/utMpEnF4hmUcR19S7</a>.</b> -->

<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      This workshop aims to bring together researchers working on 3D computer vision and graphics for eCommerce, with a focus on the three topics: (1) 3D object/scene modeling and understanding in 3D eCommerce such as semantic segmentation, affordance and motion, multi-view reconstruction; (2) human modeling and fashion in 3D eCommerce such as virtual try-ons and personalized fashion recommendation, and (3) language-assisted reasoning such as shape/scene synthesis from texts and language grounding in 3D models. We invited 6 keynote speakers from academia and 3 talks from industry experts. We will also host a challenge on 3D part labeling for 3D models from real products sold online.
     <!--This workshop aims to bring together researchers working on generative models of 3D shapes and scenes with researchers and practitioners who use these generative models in a variety of research areas. For our purposes, we define "generative model" to include methods that synthesize geometry unconditionally as well as from sensory inputs (e.g. images), language, or other high-level specifications. Vision tasks that can benefit from such models include scene classification and segmentation, 3D reconstruction, human activity recognition, robotic visual navigation, question answering, and more.-->
    </p>
  </div>
</div>
<p><br /></p>

<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Schedule</h2>
    <p>All times in Paris Time (UTC+02:00)</p>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr>
          <td>8:50am - 9:00am</td>
          <td>Welcome and Introduction (Richard Zhang)</td>
          <td></td>
        </tr>
        <tr>
          <td>9:00am - 9:35am</td>
          <td>
          Invited Talk 1 (Leonidas Guibas)
          <br />
          <i>Title: Compositional Object Modeling: Parts, Language, and Functionality</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>9:35am - 10:10am</td>
          <td>
          Invited Talk 2 (Rana Hanocka)
          <br />
          <i>Title: Data-Driven Shape Editing without 3D Data</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>10:10am - 10:20am</td>
          <td>Coffee break</td>
          <td></td>
        </tr>
        <tr>
          <td>10:20am - 10:55am</td>
          <td>
          Invited Talk 3 (Michael Black)
          <br />
          <i>Title: Implicit, Explicit, Real, and Synthetic: Spinning the Virtual Fashion Flywheel</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>10:55am - 11:30am</td>
          <td>Invited Talk 4 (Ming Lin)
          <br />
          <i>Title: Dynamics-Inspired Learning-based Virtual Try-On</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>11:30am - 12:00pm</td>
          <td>Invited student paper presentations
          <br />
          <i>HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling</i> (Fenggen Yu) <br />
          <i>TriCoLo: Trimodal Contrastive Loss for fine-grained Text to Shape Retrieval</i> (Angel Chang) <br />
          <i>Improving Unsupervised Visual Program Inference with Code Rewriting Families</i> (Aditya Ganeshan) <br />
          <i>Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior</i> (Junshu Tang) <br />
          <i>D-IF: Uncertainty-aware Human Digitization via Implicit Distribution Field</i> (Xueting Yang)
          </td>
          <td></td>
        </tr>
        <tr>
          <td>12:00pm - 1:30pm</td>
          <td>Lunch break</td>
          <td></td>
        </tr>
        <tr>
          <td>1:30pm - 2:00pm</td>
          <td>Winner presentations of the challenge</td>
          <td></td>
        </tr>
        <tr>
          <td>2:00pm - 2:35pm</td>
          <td>
          Invited Talk 5 (Roozbeh Mottaghi)
          <br />
          <i>Title: Habitat 3.0: A Co-Habitat for Humans, Avatars, and Robots</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>2:35pm - 3:10pm</td>
          <td>
          Invited Talk 6 (Katerina Fragkiadaki)
          <br />
          <i>Title: 3D Part Segmentation and Reconstruction with Little or No Training</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>3:10pm - 3:20pm</td>
          <td>Coffee break</td>
          <td></td>
        </tr>
        <tr>
          <td>3:20pm - 3:45pm</td>
          <td>
          Industry Talk 1 (Eric Bennett from Amazon)
          <br />
          <i>Title: Scaling 3D eCommerce Innovations <br /> A 10,000 Meter View on Building Durable 3D Content Creation Algorithms for Growth</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>3:45pm - 4:10pm</td>
          <td>
          Industry Talk 2 (Itamar Berger from Snap)
          <br />
          <i>Title: Snap to Fit: AR Try-on in Snapchat</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>4:10pm - 4:35pm</td>
          <td>
          Industry Talk 3 (Chengfei Lv from Alibaba)
          <br />
          <i>Title: Introduction to 3D and XR Technology in Taobao</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>4:35pm - 5:00pm</td>
          <td>Panel discussion and community building <br />
          <i>Panelists: Ming Lin, Rana Hanocka, Michael Black</i><br />
          <i>Moderator: Daniel Ritchie</i></td>
          <td></td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://geometry.stanford.edu/member/guibas/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/guibas.jpg" /></a>
    <p>
      <b><a href="https://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a></b> Paul Pigott Professor of Computer Science and Electrical Engineering at Stanford University. He heads the Geometric Computation group in the Computer Science Department. He works on algorithms for sensing, modeling, reasoning, rendering, and acting on the physical world. More recently, he has focused on shape analysis and computer vision using deep neural networks.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://people.cs.uchicago.edu/~ranahanocka/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/rana.jpg" /></a>
    <p>
      <b><a href="https://people.cs.uchicago.edu/~ranahanocka/">Rana Hanocka</a></b> Assistant Professor of Computer Science at the University of Chicago. She directs 3DL, a group of enthusiastic researchers passionate about 3D, machine learning, and visual computing. She obtained her Ph.D. in 2021 from Tel Aviv University under the supervision of Daniel Cohen-Or and Raja Giryes. Her research is focused on building artificial intelligence for 3D data, spanning the fields of computer graphics, machine learning, and computer vision.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://ps.is.mpg.de/~black"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/black.jpeg" /></a>
    <p>
      <b><a href="https://ps.is.mpg.de/~black">Michael Black</a></b> Honorary Professor at the University of Tuebingen and one of the founding directors at the Max Planck Institute for Intelligent Systems in TÃ¼bingen, Germany, where he leads the Perceiving Systems department. He was also a Distinguished Amazon Scholar (VP, 2017-2021). Black's research interests in computer vision include optical flow estimation, 3D shape models, human shape and motion analysis, robust statistical methods, and probabilistic models of the visual world. Black co-founded he co-founded Body Labs Inc., which commercialized 3D body model technology, and was acquired by Amazon.com in 2017.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://www.cs.umd.edu/~lin/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/lin.jpeg" /></a>
    <p>
      <b><a href="https://www.cs.umd.edu/~lin/">Ming Lin</a></b> Distinguished University Professor and former Elizabeth Stevinson Iribe Chair of Computer Science at the University of Maryland College Park. Her research interests include computational robotics, haptics, physically-based modeling, virtual reality, sound rendering, and geometric computing. She has (co-)authored more than 300 refereed publications in these areas and co-edited/authored four books. Many of Linâ€™s research findings have been patented and licensed by more than 50 companies world wide. Lin is an Amazon Scholar with Amazon Fashion.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://roozbehm.info/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/roozbeh.jpeg" /></a>
    <p>
      <b><a href="https://roozbehm.info/">Roozbeh Mottaghi</a></b> Research Scientist Manager at Meta and Affiliate Associate Prof. at the University of Washington working on Vision-and-Language and Embodied AI. Prior to joining FAIR, he was the Research Manager of the PRIOR team at the Allen Institute for AI. Before that, I was a Postdoctoral Researcher in the Computer Science Department at Stanford University. He was a post-doctoral researcher at the Computer Science Department at Stanford University. He obtained his PhD in Computer Science from University of California, Los Angeles.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://www.cs.cmu.edu/~katef/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/katef.png" /></a>
    <p>
      <b><a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a></b> Assistant Professor in the Machine Learning Department at Carnegie Mellon University. She received her Ph.D. from the University of Pennsylvania and was a postdoctoral fellow at UC Berkeley and Google research after that.  Her work is on learning visual representations with little supervision and combining spatial reasoning in deep visual learning. Her group develops algorithms for mobile computer vision,  learning of physics, and common sense for agents that move around and interact with the world.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://www.linkedin.com/in/eric-bennett-2664a817"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/eric.jpeg" /></a>
    <p>
      <b><a href="https://www.linkedin.com/in/eric-bennett-2664a817">Eric Bennett</a></b> Director of Science at Amazon Imaging. Bennett leads the research and development of cutting-edge solutions for the creation of 3D models using ML, CV, photogrammetry, and more to bring new immersive experiences to Amazon's customers. He obtained his Ph.D. in Computer Science from University of North Carolina at Chapel Hill.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://itamarbe.github.io/"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/itamar.jpeg" /></a>
    <p>
      <b><a href="https://itamarbe.github.io/">Itamar Berger</a></b> Computer Vision Engineering Manager at Snap. His team develops products for improving the Augmented Reality experiences using Deep Learning and Generative AI for SnapAR. Before Snap, he co-founded a statup in the field of real-time motion capture and was a R&amp;D manager at Autodesk.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://www.researchgate.net/profile/Chengfei-Lv"><img class="people-pic" style="float:left;margin-right:50px;" src="/static/img/people/chengfei.jpeg" /></a>
    <p>
      <b><a href="https://www.researchgate.net/profile/Chengfei-Lv">Chengfei Lv</a></b> Head of Alibaba's 3D/XR Technology Department. His team works on 3D/XR technologies specific to eCommerce, such as 3D reconstruction of commodities, high-performing volumetric video for MR, and 3D modeling and rendering engines, responsible for exploring innovative consumer applications for immersive experiences.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Challenge</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      <span style="font-weight:500;">Call for particpations:</span> The workshop will host a competition focused on fine-grained semantic segmentation of 3D shapes. The competition will use 3D models from five categories (chair, table, cabinet, lamp, and bed) from the Amazon-Berkeley-Object (ABO) dataset, which has a total of 3400 models. The ABO dataset, recently published, features high quality, uniformly standard 3D models of real products sold online, created by artists. The models are made up of build-aware connected components, which form the basis of various shape properties such as texture, motion, function, interaction, and construction. The workshop challenge focuses on assigning fine-grained semantic labels (e.g., leg and arm, defined based on PartNet) to these connected components in the ABO dataset. 
    </p>
    <p>
      <span style="font-weight:500;">Submission site:</span> <a href="https://eval.ai/web/challenges/challenge-page/2027/overview" target="_blank">https://eval.ai/web/challenges/challenge-page/2027/overview</a>
    </p>
  </div>
</div>

<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Release of train and validation sets</td>
          <td>June 01, 2023</td>
        </tr>
        <tr>
          <td>Release of test set</td>
          <td>Aug 15, 2023</td>
        </tr>
        <tr>
          <td>Submission deadline</td>
          <td>Sep 10, 2023</td>
        </tr>
        <tr>
          <td>Workshop date</td>
          <td>Oct 02, 2023</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<!--<br>
<div class="row" id="accepted">
  <div class="col-md-12">
    <h2>Accepted Papers</h2>
  </div>
</div>

<div class="row text-center">
  <div class="col-md-12">
    <hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0001-poster.png" width='700'><br/>
    <a href=''>3D GAN Inversion for Controllable Portrait Image Animation</a></span>
    <br/>
    <i>Connor Z. Lin, David B. Lindell, Eric R. Chan, Gordon Wetzstein</i>
    <br/>
    <a href='https://drive.google.com/file/d/18qcR7WjImq_wRpfLf2aI7cRb62JyMppY/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1ZEHbONpIk8tGnCY3PWB3gpw-Cqf-_fLt/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1HkDUt1z7M_Bv5THurz8-wewoqqkg7HFo/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0002-poster.png" width='700'><br/>
    <a href=''>3DLatNav: Navigating Generative Latent Spaces for Semantic-Aware 3D Object Manipulation</a></span>
    <br/>
    <i>Amaya Dharmasiri, Dinithi Dissanayake, Mohamed Afham, Isuru Dissanayake, Ranga Rodrigo, Kanchana Thilakarathna</i>
    <br/>
    <a href='https://drive.google.com/file/d/1ZnObAPLwCYvUCqyreMr7dWOlWEvcEL4W/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1rD_YGMNfUkVA_7-RJ6SX8y_rjPZYZlbU/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0003-poster.png" width='700'><br/>
    <a href=''>3D Semantic Label Transfer and Matching in Human-Robot Collaboration</a></span>
    <br/>
    <i>Szilvia Szeier, BenjÃ¡min Baffy, GÃ¡bor Baranyi, Joul Skaf, LÃ¡szlÃ³ KopÃ¡csi, Daniel Sonntag, GÃ¡bor SÃ¶rÃ¶s, and AndrÃ¡s LÅ‘rincz</i>
    <br/>
    <a href='https://drive.google.com/file/d/1GD065M4qj2BhT6ujZEv_kMTFTmBYWL6C/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15GsMVDqnVBQnmg-bIifgY8gENf-xtAn0/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0004-poster.png" width='700'><br/>
    <a href=''>Generative Multiplane Images: Making a 2D GAN 3D-Aware</a></span>
    <br/>
    <i>Xiaoming Zhao, Fangchang Ma, David GÃ¼era, Zhile Ren, Alexander G. Schwing, Alex Colburn</i>
    <br/>
    <a href='https://drive.google.com/file/d/13OVLhihZ5xQEuY_tTu94fZTu_U7WXOpO/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1H4X3FnV2GLhdxc4jJQD45T1EweLb3lZC/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0005-poster.png" width='700'><br/>
    <a href=''>Intrinsic Neural Fields: Learning Functions on Manifolds</a></span>
    <br/>
    <i>Lukas Koestler, Daniel Grittner, Michael Moeller, Daniel Cremers, Zorah LÃ¤hner</i>
    <br/>
    <a href='https://drive.google.com/file/d/1gfRpZL1HzAkyAVqnD-bWjWTPSNtBdlhk/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/19g5Nq3YIg8KdEkG77QN0QQpORgVeHH39/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0006-poster.png" width='700'><br/>
    <a href=''>Learning Joint Surface Atlases</a></span>
    <br/>
    <i>Theo Deprelle, Thibault Groueix, Noam Aigerman, Vladimir G. Kim, Mathieu Aubry</i>
    <br/>
    <a href='https://drive.google.com/file/d/1udFGRnASw9iLDf7PyKMCr_-oZOkU9msR/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1xlEOAWprb1TDx54MNKCVA-lx3uWXDvBn/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0007-poster.png" width='700'><br/>
    <a href=''>Learning Neural Radiance Fields from Multi-View Geometry</a></span>
    <br/>
    <i>Marco Orsingher, Paolo Zani, Paolo Medici, Massimo Bertozzi</i>
    <br/>
    <a href='https://drive.google.com/file/d/1iVmNqUEzmPrHsimYqfncO-rxpt7T-ekX/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/195vUJdaeFqCqprm6to6s_NEfo6pacMhm/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0008-poster.png" width='700'><br/>
    <a href=''>Mosaic-based omnidirectional depth estimation for view synthesis</a></span>
    <br/>
    <i>Min-jung Shin, Minji Cho, Woojune Park, Kyeongbo Kong, Joonsoo Kim, Kug-jin Yun, Gwangsoon Lee, Suk-Ju Kang</i>
    <br/>
    <a href='https://drive.google.com/file/d/1o8lQ35W7DsVWQuCsjHmRwscPe3qHX2gx/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/16LQyXp6cMG5ttMCjOKUpUUxLScdQlhPU/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0009-poster.png" width='700'><br/>
    <a href=''>Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, and Program</a></span>
    <br/>
    <i>Tiange Luo, Honglak Lee, Justin Johnson</i>
    <br/>
    <a href='https://drive.google.com/file/d/16LQyXp6cMG5ttMCjOKUpUUxLScdQlhPU/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1drW8odKGHqiZ-5Hykh6f2TsHveF8buO_/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0010-poster.png" width='700'><br/>
    <a href=''>RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis</a></span>
    <br/>
    <i>Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan Kautz, Alexander Keller, Sameh Khamis, Thomas MÃ¼eller, Charles Loop, Nathan Morrica, Koki Nagano, Towaki Takikawa, Stan Birchfield</i>
    <br/>
    <a href='https://drive.google.com/file/d/1SzKp_SD4-vyabtuo5RxMro2P6uZlWDyl/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1fVA7aTR1XbtfTepEvPSc79Zt-5lupzMt/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0011-poster.png" width='700'><br/>
    <a href=''>Recovering Detail in 3D Shapes Using Disparity Maps</a></span>
    <br/>
    <i>Marissa Ramirez de Chanlatte, Matheus Gadelha, Thibault Groueix, Radomir Mech</i>
    <br/>
    <a href='https://drive.google.com/file/d/1tZKknBI4iTdBJ_9lhZIY8nESDVDDPJFu/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15Lf7ki5o4f3YA7n6PXgfzQG5zxkJ3VNF/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0012-poster.png" width='700'><br/>
    <a href=''>Share With Thy Neighbors: Single-View Reconstruction by Cross-Instance Consistency</a></span>
    <br/>
    <i>Tom Monnier, Matthew Fisher, Alexei A. Efros, Mathieu Aubry</i>
    <br/>
    <a href='https://drive.google.com/file/d/116Ou7tuDWk6oOPaw-ng4PCM9mMz84jn-/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0013-poster.png" width='700'><br/>
    <a href=''>Towards Generalising Neural Implicit Representations</a></span>
    <br/>
    <i>Theo W. Costain, Victor A. Prisacariu</i>
    <br/>
    <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
  </div>
</div>
-->

<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row text-center">

  <div class="col-xs-2">
    <a href="https://yi-ming-qian.github.io/">
      <img class="people-pic" src="/static/img/people/yiming.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://yi-ming-qian.github.io/">Yiming Qian</a>
      <h6>Amazon</h6>
      <h6>(Primary Contact)</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://fenggenyu.github.io/">
      <img class="people-pic" src="/static/img/people/fenggen.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://fenggenyu.github.io/">Fenggen Yu</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://panchagil.github.io/">
      <img class="people-pic" src="/static/img/people/pancha.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://panchagil.github.io/">Francisca Gil-Ureta</a>
      <h6>Amazon</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://xu-zhang-1987.github.io/">
      <img class="people-pic" src="/static/img/people/xu.jpg" />
    </a>
    <div class="people-name">
      <a href="https://xu-zhang-1987.github.io/">Xu Zhang</a>
      <h6>Amazon</h6>
    </div>
  </div>


  <div class="col-xs-2">
    <a href="https://jazcollins.github.io/">
      <img class="people-pic" src="/static/img/people/jasmine.png" />
    </a>
    <div class="people-name">
      <a href="https://jazcollins.github.io/">Jasmine Collins</a>
      <h6>UC Berkeley</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://angelxuanchang.github.io/">
      <img class="people-pic" src="/static/img/people/angel.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://angelxuanchang.github.io/">Angel Chang</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

</div>

<div class="row text-center">

  <div class="col-xs-2">
    <a href="https://egundogdu.github.io/">
      <img class="people-pic" src="/static/img/people/erhan.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://egundogdu.github.io/">Erhan Gundogdu</a>
      <h6>Amazon</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://dritchie.github.io/">
      <img class="people-pic" src="/static/img/people/daniel.png" />
    </a>
    <div class="people-name">
      <a href="https://dritchie.github.io/">Daniel Ritchie</a>
      <h6>Brown University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://ps.is.mpg.de/~jromero">
      <img class="people-pic" src="/static/img/people/javier.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://ps.is.mpg.de/~jromero">Javier Romero</a>
      <h6>Meta</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://jianwang-cmu.github.io/">
      <img class="people-pic" src="/static/img/people/jian.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://jianwang-cmu.github.io/">Jian Wang</a>
      <h6>Snap</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://hufu6371.github.io/huanfu/">
      <img class="people-pic" src="/static/img/people/huan.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://hufu6371.github.io/huanfu/">Huan Fu</a>
      <h6>Alibaba</h6>
    </div>
  </div>

  

  

</div>

<hr />

<h2>Senior Organizers</h2>
<div class="row text-center">
  <div class="col-xs-2">
    <a href="https://lorisbaz.github.io/">
      <img class="people-pic" src="/static/img/people/loris.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://lorisbaz.github.io/">Loris Bazzani</a>
      <h6>Amazon</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.linkedin.com/in/devernay">
      <img class="people-pic" src="/static/img/people/fred.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://www.linkedin.com/in/devernay">Frederic Devernay</a>
      <h6>Amazon</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.cs.sfu.ca/~haoz/">
      <img class="people-pic" src="/static/img/people/hao.jpg" />
    </a>
    <div class="people-name">
      <a href="https://www.cs.sfu.ca/~haoz/">Hao (Richard) Zhang</a>
      <h6>Simon Fraser University &amp; Amazon</h6>
    </div>
  </div>
</div>

<p><br />
<br /></p>

<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<p><a name="/acknowledgements"></a></p>
<div class="row">
  <div class="col-xs-12">
    <p>
      We thank <span style="color:#1a1aff;font-weight:400;"> <a href="https://www.guillaum.in/~matthieu/">Matthieu Guillaumin</a></span> for his assistance in data uploading and for driving the ABO dataset release! We thank <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> for the webpage format.
    </p>
  </div>
</div>

<p><br /></p>


      </div>
    </div>

    

    <script type="text/javascript" src="/static/js/jquery.min.js"></script>
    <script type="text/javascript" src="/static/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="/static/js/main.js"></script>
  </body>
</html>
